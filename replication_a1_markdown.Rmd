---
title: "Replication Econometrics III -- A1"
author: "Calebe Piacentini and Giovanni"
date: "2024-01-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(quantmod)
library(forecast)
library(lubridate)
library(gt)
library(tibble)
library(knitr)
library(kableExtra)
```

## What do we need to replicate?

As exposed in the guidelines, we need to replicate all tables and figures **except** for the parts involving models RGM, VAR, RGMVAR, MDL1 and MDL2.

Altering table 2 from the paper to exclude these models, we have:

```{r models}
# Create the data frame
models_table <- data.frame(
 Description = c(
  "Time-series models", # 1
  "", # 2
  "", # 3
  "", # 4
  "Phillips curve (OLS)", # 5
  "", # 6
  "", # 7
  "", # 8
  "", # 9
  "", # 10
  "", # 11
  "", # 12
  "", # 13
  "", # 14
  "OLS term structure models", # 15
  "", # 16
  "", # 17
  "", # 18
  "", # 19
  "", # 20
  "", # 21
  "", # 22
  "", # 23
  "", # 24
  "", # 25
  "Inflation surveys", # 26
  "", # 27
  "", # 28
  "", # 29
  "", # 30
  "", # 31
  "", # 32
  "", # 33
  "" # 34
 ),
 Abbreviation = c(
  "ARMA", # 1
  "AR", # 2
  "RW", # 3
  "AORW", # 4
  "PC1", # 5
  "PC2", # 6
  "PC3", # 7
  "PC4", # 8
  "PC5", # 9
  "PC6", # 10
  "PC7", # 11
  "PC8", # 12
  "PC9", # 13
  "PC10", # 14
  "TS1", # 15
  "TS2", # 16
  "TS3", # 17
  "TS4", # 18
  "TS5", # 19
  "TS6", # 20
  "TS7", # 21
  "TS8", # 22
  "TS9", # 23
  "TS10", # 24
  "TS11", # 25
  "SPF1", # 26
  "SPF2", # 27
  "SPF3", # 28
  "LIV1", # 29
  "LIV2", # 30
  "LIV3", # 31
  "MICH1", # 32
  "MICH2", # 33
  "MICH3" # 34
 ),
 Specification = c(
  "ARMA(1,1)", # 1
  "Autoregressive model", # 2
  "Random walk on quarterly inflation", # 3
  "Random walk on annual inflation", # 4
  "INFL + GDPG", # 5
  "INFL + GAP1", # 6
  "INFL + GAP2", # 7
  "INFL + LSHR", # 8
  "INFL + UNEMP", # 9
  "INFL + XLI", # 10
  "INFL + XLI-2", # 11
  "INFL + FAC", # 12
  "INFL + GAP1 + LSHR", # 13
  "INFL + GAP2 + LSHR", # 14
  "INFL + GDPG + RATETS", # 15
  "INFL + GAP1 + RATETS", # 16
  "INFL + GAP2 + RATETS", # 17
  "INFL + LSHR + RATETS", # 18
  "INFL + UNEMP + RATETS", # 19
  "INFL + XLI + RATETS", # 20
  "INFL + XLI-2 + RATETS", # 21
  "INFL + FAC + RATETS", # 22
  "INFL + SPDTS", # 23
  "INFL + RATE + SPDTS", # 24
  "INFL + GDPG + RATE + SPDT", # 25
  "VAR(1) on RATE, SPD, INFL, GDPG", # 26
  "Linear bias-corrected SPF", # 27
  "Non-linear bias-corrected SPFLIV1", # 28
  "Livingston survey", # 29
  "Linear bias-corrected Livingston", # 30
  "Non-linear bias-corrected Livingston", # 31
  "Michigan survey", # 32
  "Linear bias-corrected Michigan", # 33
  "Non-linear bias-corrected Michigan" # 34
 )
)

knitr::kable(models_table)
```


## Loading Data

Data from the paper.

Part of the data (mainly from real data sources and when it comes from other papers) was gathered through the authors data replication kit (asked by e-mail). 

```{r load_data}

# todo: missing data from real activity
"
FRED:
1. GDP
=> output gap (recursive filters):
  a. GAP1: remove quadratic trend (Gali and Gertler, 1999)
  b. GAP2: HP filter (param_smooth=1600)
2. Unemployment


"

process_inflation_data <- function(file_path, inflation_label, period_column = "Period", label_column = "Label", value_column = "Value") {
  df <- read_csv(file_path) %>%
    select(Period = period_column, Label = label_column, P = value_column) %>%
    mutate(
      DATE = dmy(paste("01", substr(Period, 2, 3), substr(Label, 1, 4), sep = "-")), # Generate DATE column early to use for arrangement
      P_lag = lag(P, n =3), # Create a lagged version of the price column
      !!inflation_label := log(P / P_lag) # Dynamically name the inflation column
    ) %>%
    na.omit() %>%
    arrange(DATE) %>%
    mutate(Quarter = paste(year(DATE), quarter(DATE), sep = "-Q")) %>%
    group_by(Quarter) %>%
    summarise(
      FirstDate = first(DATE),
      FirstP = first(P),
      FirstP_lag = first(P_lag),
      !!inflation_label := first(!!sym(inflation_label))
    ) %>%
    select(Quarter, FirstDate, FirstP, FirstP_lag, !!inflation_label)
}

# Process each dataset
punew_df <- process_inflation_data("data/punew_1947-2023.csv", "inflation_punew")
puxhs_df <- process_inflation_data("data/puxhs_1947-2023.csv", "inflation_puxhs")
puxx_df <- process_inflation_data("data/puxx_1957-2023.csv", "inflation_puxx")

rm(process_inflation_data)

pce_df <- read_csv("data/pce_DPCERD3Q086SBEA_1947-2023.csv") %>%
  rename(P = "DPCERD3Q086SBEA") %>%
  mutate(
    P_lag = lag(P),
    inflation_pce = log(P / P_lag),
    FirstDate = DATE
  ) %>%
  na.omit()


# Merge all dataframes on the DATE column
merged_df <- reduce(list(punew_df, puxhs_df, puxx_df, pce_df), full_join, by = "FirstDate") %>%
  select("FirstDate", contains("inflation")) %>%
  mutate(quarter = paste(year(FirstDate), quarter(FirstDate), sep = "-Q")) 

# write.csv(merged_df, file = "inflation_panel.csv")

rm(pce_df, punew_df, puxhs_df, puxx_df)

## 02.1 Recreate table 1 Summary Statistics - original --------------------------

### Filter the data based on the specified date ranges for each series ----------
punew_puxhs_filter <- filter(merged_df, FirstDate >= as.Date("1952-04-01") & FirstDate <= as.Date("2002-10-01")) %>%
  select("FirstDate", "inflation_punew", "inflation_puxhs", "quarter")
puxx_filter <- filter(merged_df, FirstDate >= as.Date("1958-04-01") & FirstDate <= as.Date("2002-10-01")) %>%
  select("FirstDate", "inflation_puxx", "quarter")
pce_filter <- filter(merged_df, FirstDate >= as.Date("1960-04-01") & FirstDate <= as.Date("2002-10-01")) %>%
  select("FirstDate", "inflation_pce", "quarter")

# Join the dataframes
df_inflation_authors <- full_join(punew_puxhs_filter, puxx_filter, by = "FirstDate") %>%
  full_join(., pce_filter, by = "FirstDate")  %>%
  mutate(group = year(FirstDate)) %>%
  select("FirstDate", "inflation_punew", "inflation_puxhs", "inflation_puxx", "inflation_pce", "quarter" = "quarter.x", "group") 

punew_year <- df_inflation_authors %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

rm(joined_df, pce_filter, punew_puxhs_filter, puxx_filter)

original_year_df <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))
  
rm(pce_year, punew_year, puxhs_year, puxx_year)


```

And now data from Brazil.

## Summary statistics

Here we replicate table 1 of Summary Statistics from the paper and the following figures.

First, we do it for the exactly the period of their analysis. It's worth noting that although their first year of analysis is 1952, PUXX only starts to have data at 1957 (even though they use data starting from 1958).

```{r summary_statistics_data_original}
### Panel A --------------------------------------------------

data_variables <- select(original_year_df, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_a <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))
  
# New column to add at the start
statistics_labels <- c("Mean", "Standard deviation", "Autocorrelation", "Correlations", "PUXHS", "PUXX", "PCE")

# Adding the new column at the start of the dataframe
panel_a <- panel_a %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
kable(panel_a, "latex", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel A: 1952:Q2–2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_a, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel B -----------------------------------------------

df_inflation_authors_B <- df_inflation_authors %>%
  filter(FirstDate >= "1986-01-01" & FirstDate <= "2002-10-01")

punew_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

original_year_df_B <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(original_year_df_B, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors_B %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_b <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_b <- panel_b %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
kable(panel_b, "latex", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel B: 1986:Q1– 2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_b, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel C -----------------------------------------------

df_inflation_authors_C <- df_inflation_authors %>%
  filter(FirstDate >= "1996-01-01" & FirstDate <= "2002-10-01")

punew_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

original_year_df_C <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(original_year_df_C, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors_C %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_c <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_c <- panel_c %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
kable(panel_c, "latex", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel C: 1996:Q1– 2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_c, sds, means, autocorrelation_quaterly, corr_table, data_variables)

rm(df_inflation_authors_B, df_inflation_authors_C, original_year_df_B, original_year_df_C, statistics_labels)
## 0.2.2 Recreate figure 1.A -----------------------------

# Multiply the inflation columns by 100
data_plot1A <- df_inflation_authors %>%
  mutate(across(starts_with("inflation"), ~ .x * 100))

# Pivot the data to a long format for plotting with ggplot2
data_plot1A_long <- data_plot1A %>%
  pivot_longer(cols = starts_with("inflation"), names_to = "inflation_type", values_to = "inflation_value")

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "longdash", "dotted", "dotdash")
shapes <- c(NA, NA, NA, 3) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to linetypes
names(line_types) <- unique(data_plot1A_long$inflation_type)
names(shapes) <- unique(data_plot1A_long$inflation_type)

# Plot the data
plot_1a <- ggplot(data_plot1A_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_date(date_breaks = "5 years", date_labels = "%Y") 

plot_1a
# ggsave("inflation_time_series.png", plot_1a, width = 11, height = 8.5)

```
## Estimating the models

### Time-Series Models

Here we'll estimate the ARMA, AR, RW and AORW.

For the forecasts

```{r time_series_models}
# Function to perform rolling window forecasting with ARMA(1,1) over years
rolling_forecast_arma <- function(data, series_name, order_ar=1, order_ma=1, initial_end=as.Date('1985-12-01'), final_date=as.Date('2002-12-01'), window_size=4, return_rmse=T) {
  # adding a forecast vector to store results
  forecasts <- numeric((year(final_date) - year(initial_end))*window_size)
  
  # loop for the rolling window
  for (y in year(initial_end):year(final_date)) {
    train_data <- data %>% filter(FirstDate <= as.Date(str_c(y, '-12-01')))  # Use data up to the current index for training
    train_series <- train_data %>% .[series_name]
    
    # Fit ARMA(1,1) model: yt = mu + phi*yt-1 + epsilon_t + psi*epsilon_{t-1}
    arma_model <- arima(train_series, order = c(order_ar, 0, order_ma))
    # get coefficients + residuals
    phi <- arma_model$coef[1]
    psi <- arma_model$coef[2]
    mu <- arma_model$coef[3]
    residuals <- arma_model$residuals
    
    # Forecast for the next 4 quarters according to equation on paper
    forecast_q <- c()
    for (q in 1:4) {
      forecast_q[q] <- (1/(1-phi)) * (4 - (phi * (1-phi^4))/(1-phi)) * mu + ((phi * (1-phi^4))/(1-phi)) * train_series[nrow(train_series) - (q-1),] + (psi * (1-phi^4))/(1-phi) * residuals[nrow(train_series) - (q-1)]
    }
    
    # just making sure
    forecast_values <- as.numeric(forecast_q)
    # Store forecast values
    forecasts[((y - year(initial_end))*window_size + 1):((y - year(initial_end)+1)*window_size)] <- forecast_values
  }
  
  # decide whether to return forecasts or root mean squared errors (RMSE)
  if (return_rmse) {
    # get data to validate forecasts
    test_series <- data %>% filter(FirstDate > as.Date(initial_end)) %>% .[series_name]
    rmse <- sqrt(sum((test_series - forecasts)^2))
    return(rmse)
  } else{
    return(forecasts)
  }
}

# very similar to the last function, 
# with the difference that now we'll choose AR(p) recursively by SIC
rolling_forecast_ar_p <- function(data, series_name, initial_end=as.Date('1985-12-01'), final_date=as.Date('2002-12-01'), window_size=4, return_rmse=T) {
  # creating a forecast vector to store results
  forecasts <- numeric((year(final_date) - year(initial_end))*window_size)
  
  # loop for the rolling window
  for (y in year(initial_end):year(final_date)) {
    train_data <- data %>% filter(FirstDate <= as.Date(str_c(y, '-12-01')))  # Use data up to the current index for training
    train_series <- train_data %>% select(series_name)
    
    # find best ar order
    ar_order <- best_arma_order(train_series)
    # Fit AR(p) model
    arma_model <- stats::arima(train_series, order = c(ar_order, 0, 0))
    
    # Forecast for the next 4 quarters
    #  ... a little note: here we abstract from the way the paper is doing, since it would imply unecessary complexity (look at the equation for the quarterly forecast in page 1174)
    forecast_values <- as.numeric(forecast(arma_model, h = 4)$mean)
    # Store forecast values
    forecasts[((y - year(initial_end))*window_size + 1):((y - year(initial_end)+1)*window_size)] <- forecast_values
  }
  
  # decide whether to return forecasts or root mean squared errors (RMSE)
  if (return_rmse) {
    # get data to validate forecasts
    test_series <- data %>% filter(FirstDate > as.Date(initial_end)) %>% .[series_name]
    rmse <- sqrt(sum((test_series - forecasts)^2))
    return(rmse)
  } else{
    return(forecasts)
  }
}

# to find the best arma model based on some criteria (still don't contemplate ma orders or other info_criteria than BIC/SIC)
best_arma_order <- function(series, ar_until = 10, ma_order = 0, info_criteria = 'bic'){
  arma_models <- list()
  bic_arma <- c()
  for (p in 1:ar_until) {
    arma_models[[p]] <- arima(series, order = c(p, 0, ma_order))
    # BIC = SIC
    bic_arma[p] <- BIC(arma_models[[p]])
  }
  arma_best_order <- which(min(bic_arma) == bic_arma)
  return(arma_best_order)
}

# running the functions
inflation_series <- merged_df %>% select(starts_with('inflation')) %>% colnames()

arma11_rmse <- c()
arp_rmse <- c()
for (x in 1:length(inflation_series)) {
  arma11_rmse[x] <- rolling_forecast_arma(df_inflation_authors, inflation_series[x])
  arp_rmse[x] <- rolling_forecast_ar_p(df_inflation_authors, inflation_series[x])
}

rolling_forecast_arma(df_inflation_authors, inflation_series[1], return_rmse = T)

```

### Phillips Curve (OLS)

First we load inflation data and real activity measures.

```{r load_data_phillips}
# Load data
# df_inflation_complete <- read.csv("df_inflation_complete.csv")
df_inflation_authors <- read.csv("df_inflation_authors.csv")
df_realmeasures_authors <- read.csv("df_realmeasures_authors.csv")

# Prepare data
df_phillips <- df_inflation_authors %>%
  left_join(df_realmeasures_authors) %>% 
  tibble()

y <- 'inflation_punew'
x <- 'gdpg'
teste <- df_phillips
for (p in 1:3) {
  teste <- teste %>% 
    mutate(!!str_c('lag', p, '_', y) := dplyr::lag(!!sym(y), p),
           !!str_c('lag', p, '_', x) := dplyr::lag(!!sym(x), p))
}

dependent_var <- y
independent_var <- teste %>% select(starts_with('lag1')) %>% colnames()

for (p in 2:3) {
  independent_var_plus <- teste %>% select(starts_with(paste0('lag', p))) %>% colnames()
  independent_var <- c(independent_var, independent_var_plus)
}

reg_formula <- formula(paste(dependent_var, '~', paste(independent_var, collapse=' + ')))
model <- lm(reg_formula, data = teste)

```

Then, we do a similar procedure as before for the time-series models, however, now we estimate the models using a simple OLS regression. As for the AR(p) model we'll dynamically choose the order of the regressors such that it minimizes the bayesian information criteria (BIC or SIC). Basically, the model we'll estimate goes as follows (for the training sample):

$$
\pi_{t+4,4} = \alpha + \beta'(L) X_t + \epsilon_{t+4,4}
$$
where $\pi_{t+4,4}$ is defined (annually) as previously, $\beta(L)$ is a filter for the variables in $X_t$ that includes the dependent variable (inflation) and another independent variable (some real activity measure, $y_{t+4,4}$). That means that if we have $L=2$, $\beta'(L) X_t$ will include $\pi_{t+3,4}$, $\pi_{t+2,4}$, $y_{t+3,4}$ and $y_{t+2,4}$.

Regarding the window of training and testing, we weren't able to identify whether the authors keep, as before, a rolling window or whether they now use a fixed one. Despite it would make sense to keep the previous approach, we choose to use a fixed window to keep with a conservative/simplicity take.

```{r phillips}
# a function just to lag our dataset
lag_df <- function(df, y, x, lag_until=5){
  models <- list()
  bic_models <- c()
  df_lag <- df
  # alter the data to contain the lags you want
  for (p in 1:lag_until) {
  df_lag <- df_lag %>% 
    mutate(!!str_c('lag', p, '_', y) := dplyr::lag(!!sym(y), p),
           !!str_c('lag', p, '_', x) := dplyr::lag(!!sym(x), p))
  }
  return(df_lag)
}

# a function to choose the best lag of the ols based on bic
best_lag_ols <- function(df_lag, y, lag_until=5, criteria='bic'){
  # defining relevant variables
  dependent_var <- y
  independent_var <- c()
  
  # variables to store
  models <- list()
  bic_models <- c()
  
  # loop of lags
  for (p in 1:lag_until) {
    independent_var_plus <- df_lag %>% select(starts_with(paste0('lag', p))) %>% colnames()
    independent_var <- c(independent_var, independent_var_plus)
    reg_formula <- formula(paste(dependent_var, '~', paste(independent_var, collapse=' + ')))
    models[[p]] <- lm(reg_formula, data=df_lag)
    bic_models[p] <- BIC(models[[p]])
  }
  
  # then find the best order
  best_order <- which(min(bic_models) == bic_models)
  
  return(models[[best_order]])
}

# todo: use a rolling window?
# a function to predict for a ols model
ols_predict <- function(train_data, test_data, y, x, max_lag=5){
  # choose the best lag and returns the model
  model <- best_lag_ols(train_data, y, lag_until = max_lag)
  
  # forecast values
  forecast_values <- forecast(model, test_data)
  
  return(forecast_values$mean)
}

all_y <- df_phillips %>% select(starts_with('inflation')) %>% colnames()  # "inflation_punew" "inflation_puxhs" "inflation_puxx"  "inflation_pce"
all_x <-df_phillips %>% select(gdpg:fac) %>% colnames()  # "gdpg"            "gap1"            "gap2"            "unrate"          "lshr"            "xli"             "xli2"            "fac"

# todo: we need more one loop for the other group: 1995

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
for (y in all_y) {
  print(y)
  for (x in all_x) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= 1985)
    test_data <- df_lag %>% filter(group > 1985)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2)))
    
    # compute 1-lambda
    # model_lambda
    # first with HH-error
    # then with NW-error
    
  }
}

# todo: think how to do this table
# storing data for table
rmse_phillips <- matrix(NA, length(all_y), length(all_x))  # then we just complete and the order of completetion first run by rows then by columns
rmse_phillips[1:nrow(rmse_phillips)*ncol(rmse_phillips)] <- rmse

# separately we run the other two models


# test: ok, except the fact that we have NAs in the end, which giovanni actually already corrected
rmse <- c()
y <- 'inflation_punew'
x <- 'gdpg'
# define all the dataframes relevant
df_lag <- lag_df(df_phillips, y, x)
train_data <- df_lag %>% filter(group <= 1985)
test_data <- df_lag %>% filter(group > 1985)

# get forecast
forecast_values <- ols_predict(train_data, test_data, y, x)

# todo: store the arma11 rmse from previous exercise
# compute (relative) rmse
rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2)))


```



```{r testes}

model <- arima(df_inflation_authors['inflation_punew'], order = c(1,0,1))
model$coef


data = df_inflation_authors
series_name = 'inflation_punew'
order_ar=1
order_ma=1
initial_end=as.Date('1985-12-01')
final_date=as.Date('2002-12-01')
window_size=4



forecasts <- numeric((year(final_date) - year(initial_end))*window_size)
  
# loop for the rolling window
for (y in year(initial_end):year(final_date)) {
  train_data <- data %>% filter(FirstDate <= as.Date(str_c(y, '-12-01')))
  test_data <- data %>% filter(FirstDate > as.Date(str_c(y, '-12-01')))
  #
  train_series <- train_data %>% select(series_name)
  
  # Fit ARMA(p,q) model
  arma_model <- arima(train_series, order = c(order_ar, 0, order_ma))
  
  # Forecast for the next 4 quarters
  forecast_result <- forecast(arma_model, h = 4)
  
  # Extract the forecasted values for the next 4 quarters
  forecast_values <- as.numeric(forecast_result$mean)
  
  # Store the forecasted values
  forecasts[((y - year(initial_end))*window_size + 1):((y - year(initial_end)+1)*window_size)] <- forecast_values
}

```


## Critical part

First of all, it would be nice to have a replication package, since this would make verifiability easier (and avoid us the trouble). This is particularly troublesome since the whole point of the paper is to be quantitative, that is, making the best prediction.

However, on top of that, the paper is frequently confusing in terms of the temporal choices for their variables. Probably trying to get more data by using a quaterly frequency, they made the paper more confusing then it would be so by using estimates on an annual basis. Nevertheless, probably in daily forecasting activities data forecast are done more on a quarterly basis, which gives practitioners good advice as how to perform it.

Finally, seasonality ...

Why in the real activity measures we don't let it affect contemporaneously.
