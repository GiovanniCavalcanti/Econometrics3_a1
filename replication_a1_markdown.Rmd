---
title: "Replication Econometrics III -- A1"
author: "Calebe Piacentini and Giovanni"
date: "2024-01-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(quantmod)
library(forecast)
library(lubridate)
library(gt)
library(tibble)
library(knitr)
library(kableExtra)
```

## What do we need to replicate?

As exposed in the guidelines, we need to replicate all tables and figures **except** for the parts involving models RGM, VAR, RGMVAR, MDL1 and MDL2.

Altering table 2 from the paper to exclude these models, we have:

```{r models}
# Create the data frame
models_table <- data.frame(
 Description = c(
  "Time-series models", # 1
  "", # 2
  "", # 3
  "", # 4
  "Phillips curve (OLS)", # 5
  "", # 6
  "", # 7
  "", # 8
  "", # 9
  "", # 10
  "", # 11
  "", # 12
  "", # 13
  "", # 14
  "OLS term structure models", # 15
  "", # 16
  "", # 17
  "", # 18
  "", # 19
  "", # 20
  "", # 21
  "", # 22
  "", # 23
  "", # 24
  "", # 25
  "Inflation surveys", # 26
  "", # 27
  "", # 28
  "", # 29
  "", # 30
  "", # 31
  "", # 32
  "", # 33
  "" # 34
 ),
 Abbreviation = c(
  "ARMA", # 1
  "AR", # 2
  "RW", # 3
  "AORW", # 4
  "PC1", # 5
  "PC2", # 6
  "PC3", # 7
  "PC4", # 8
  "PC5", # 9
  "PC6", # 10
  "PC7", # 11
  "PC8", # 12
  "PC9", # 13
  "PC10", # 14
  "TS1", # 15
  "TS2", # 16
  "TS3", # 17
  "TS4", # 18
  "TS5", # 19
  "TS6", # 20
  "TS7", # 21
  "TS8", # 22
  "TS9", # 23
  "TS10", # 24
  "TS11", # 25
  "SPF1", # 26
  "SPF2", # 27
  "SPF3", # 28
  "LIV1", # 29
  "LIV2", # 30
  "LIV3", # 31
  "MICH1", # 32
  "MICH2", # 33
  "MICH3" # 34
 ),
 Specification = c(
  "ARMA(1,1)", # 1
  "Autoregressive model", # 2
  "Random walk on quarterly inflation", # 3
  "Random walk on annual inflation", # 4
  "INFL + GDPG", # 5
  "INFL + GAP1", # 6
  "INFL + GAP2", # 7
  "INFL + LSHR", # 8
  "INFL + UNEMP", # 9
  "INFL + XLI", # 10
  "INFL + XLI-2", # 11
  "INFL + FAC", # 12
  "INFL + GAP1 + LSHR", # 13
  "INFL + GAP2 + LSHR", # 14
  "INFL + GDPG + RATETS", # 15
  "INFL + GAP1 + RATETS", # 16
  "INFL + GAP2 + RATETS", # 17
  "INFL + LSHR + RATETS", # 18
  "INFL + UNEMP + RATETS", # 19
  "INFL + XLI + RATETS", # 20
  "INFL + XLI-2 + RATETS", # 21
  "INFL + FAC + RATETS", # 22
  "INFL + SPDTS", # 23
  "INFL + RATE + SPDTS", # 24
  "INFL + GDPG + RATE + SPDT", # 25
  "VAR(1) on RATE, SPD, INFL, GDPG", # 26
  "Linear bias-corrected SPF", # 27
  "Non-linear bias-corrected SPFLIV1", # 28
  "Livingston survey", # 29
  "Linear bias-corrected Livingston", # 30
  "Non-linear bias-corrected Livingston", # 31
  "Michigan survey", # 32
  "Linear bias-corrected Michigan", # 33
  "Non-linear bias-corrected Michigan" # 34
 )
)

knitr::kable(models_table)
```

First we'll replicate the authors work with similar data and then we'll do the same but for the Brazilian data.

## Loading Data

Data from the paper.

Part of the data (mainly from real data sources and when it comes from other papers) was gathered through the authors data replication kit (asked by e-mail).

We'll do the following procedure: first we gather data, save it to new files and then for each part we load the data again.

```{r load_data}

"
FRED:
1. GDP
=> output gap (recursive filters):
  a. GAP1: remove quadratic trend (Gali and Gertler, 1999)
  b. GAP2: HP filter (param_smooth=1600)
2. Unemployment


"

process_inflation_data <- function(file_path, inflation_label, period_column = "Period", label_column = "Label", value_column = "Value") {
  df <- read_csv(file_path) %>%
    select(Period = period_column, Label = label_column, P = value_column) %>%
    mutate(
      DATE = dmy(paste("01", substr(Period, 2, 3), substr(Label, 1, 4), sep = "-")), # Generate DATE column early to use for arrangement
      P_lag = lag(P, n =3), # Create a lagged version of the price column
      !!inflation_label := log(P / P_lag) # Dynamically name the inflation column
    ) %>%
    na.omit() %>%
    arrange(DATE) %>%
    mutate(Quarter = paste(year(DATE), quarter(DATE), sep = "-Q")) %>%
    group_by(Quarter) %>%
    summarise(
      FirstDate = first(DATE),
      FirstP = first(P),
      FirstP_lag = first(P_lag),
      !!inflation_label := first(!!sym(inflation_label))
    ) %>%
    select(Quarter, FirstDate, FirstP, FirstP_lag, !!inflation_label)
}

# Process each dataset
punew_df <- process_inflation_data("data/punew_1947-2023.csv", "inflation_punew")
puxhs_df <- process_inflation_data("data/puxhs_1947-2023.csv", "inflation_puxhs")
puxx_df <- process_inflation_data("data/puxx_1957-2023.csv", "inflation_puxx")

rm(process_inflation_data)

pce_df <- read_csv("data/pce_DPCERD3Q086SBEA_1947-2023.csv") %>%
  rename(P = "DPCERD3Q086SBEA") %>%
  mutate(
    P_lag = lag(P),
    inflation_pce = log(P / P_lag),
    FirstDate = DATE
  ) %>%
  na.omit()


# Merge all dataframes on the DATE column
merged_df <- reduce(list(punew_df, puxhs_df, puxx_df, pce_df), full_join, by = "FirstDate") %>%
  select("FirstDate", contains("inflation")) %>%
  mutate(quarter = paste(year(FirstDate), quarter(FirstDate), sep = "-Q")) 

# write.csv(merged_df, file = "inflation_panel.csv")

rm(pce_df, punew_df, puxhs_df, puxx_df)

## 02.1 Recreate table 1 Summary Statistics - original --------------------------

### Filter the data based on the specified date ranges for each series ----------
punew_puxhs_filter <- filter(merged_df, FirstDate >= as.Date("1952-04-01") & FirstDate <= as.Date("2002-10-01")) %>%
  select("FirstDate", "inflation_punew", "inflation_puxhs", "quarter")
puxx_filter <- filter(merged_df, FirstDate >= as.Date("1958-04-01") & FirstDate <= as.Date("2002-10-01")) %>%
  select("FirstDate", "inflation_puxx", "quarter")
pce_filter <- filter(merged_df, FirstDate >= as.Date("1960-04-01") & FirstDate <= as.Date("2002-10-01")) %>%
  select("FirstDate", "inflation_pce", "quarter")

# Join the dataframes
df_inflation_authors <- full_join(punew_puxhs_filter, puxx_filter, by = "FirstDate") %>%
  full_join(., pce_filter, by = "FirstDate")  %>%
  mutate(group = year(FirstDate)) %>%
  select("FirstDate", "inflation_punew", "inflation_puxhs", "inflation_puxx", "inflation_pce", "quarter" = "quarter.x", "group") 

punew_year <- df_inflation_authors %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

rm(joined_df, pce_filter, punew_puxhs_filter, puxx_filter)

original_year_df <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))
  
rm(pce_year, punew_year, puxhs_year, puxx_year)


```

And now data from Brazil.

## Summary statistics

Here we replicate table 1 of Summary Statistics from the paper and the following figures.

First, we do it for the exactly the period of their analysis. It's worth noting that although their first year of analysis is 1952, PUXX only starts to have data at 1957 (even though they use data starting from 1958).

```{r summary_statistics_data_original}
### Panel A --------------------------------------------------

data_variables <- select(original_year_df, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_a <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))
  
# New column to add at the start
statistics_labels <- c("Mean", "Standard deviation", "Autocorrelation", "Correlations", "PUXHS", "PUXX", "PCE")

# Adding the new column at the start of the dataframe
panel_a <- panel_a %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
kable(panel_a, "latex", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel A: 1952:Q2â€“2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_a, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel B -----------------------------------------------

df_inflation_authors_B <- df_inflation_authors %>%
  filter(FirstDate >= "1986-01-01" & FirstDate <= "2002-10-01")

punew_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

original_year_df_B <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(original_year_df_B, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors_B %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_b <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_b <- panel_b %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
kable(panel_b, "latex", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel B: 1986:Q1â€“ 2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_b, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel C -----------------------------------------------

df_inflation_authors_C <- df_inflation_authors %>%
  filter(FirstDate >= "1996-01-01" & FirstDate <= "2002-10-01")

punew_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

original_year_df_C <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(original_year_df_C, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors_C %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_c <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_c <- panel_c %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
kable(panel_c, "latex", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel C: 1996:Q1â€“ 2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_c, sds, means, autocorrelation_quaterly, corr_table, data_variables)

rm(df_inflation_authors_B, df_inflation_authors_C, original_year_df_B, original_year_df_C, statistics_labels)
## 0.2.2 Recreate figure 1.A -----------------------------

# Multiply the inflation columns by 100
data_plot1A <- df_inflation_authors %>%
  mutate(across(starts_with("inflation"), ~ .x * 100))

# Pivot the data to a long format for plotting with ggplot2
data_plot1A_long <- data_plot1A %>%
  pivot_longer(cols = starts_with("inflation"), names_to = "inflation_type", values_to = "inflation_value")

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "longdash", "dotted", "dotdash")
shapes <- c(NA, NA, NA, 3) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to linetypes
names(line_types) <- unique(data_plot1A_long$inflation_type)
names(shapes) <- unique(data_plot1A_long$inflation_type)

# Plot the data
plot_1a <- ggplot(data_plot1A_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_x_date(date_breaks = "5 years", date_labels = "%Y") 

plot_1a
# ggsave("inflation_time_series.png", plot_1a, width = 11, height = 8.5)

```

## Estimating the models

### Time-Series Models

Here we'll estimate the ARMA, AR, RW and AORW.

```{r time_series_models}

# Load dataframes used ------------------------

df_inflation_complete <- read.csv(file = "data/output/df_inflation_complete.csv")
df_inflation_authors <- read.csv(file = "data/output/df_inflation_authors.csv")
df_inflation_authors_yearly <- read.csv(file = "data/output/df_inflation_authors_yearly.csv")

# Models ------------------------------------

# Function to perform rolling window forecasting with ARMA(1,1) over years
rolling_forecast_arma <- function(data, series_name, order_ar=1, order_ma=1, initial_end=1985, final_date=2002, window_size=4, return_rmse=T) {
  # adding a forecast vector to store results
  forecasts <- numeric((final_date - initial_end)*window_size)
  
  # loop for the rolling window
  for (y in initial_end:(final_date-1)) {  # we take one out to don't count the last year
    train_data <- data %>% filter(group <= y)  # Use data up to the current index for training
    train_series <- train_data %>% .[series_name]
    
    # Fit ARMA(1,1) model: yt = mu + phi*yt-1 + epsilon_t + psi*epsilon_{t-1}
    arma_model <- arima(train_series, order = c(order_ar, 0, order_ma))
    # get coefficients + residuals
    phi <- arma_model$coef[1]
    psi <- arma_model$coef[2]
    mu <- arma_model$coef[3]
    residuals <- arma_model$residuals
    
    # Forecast for the next 4 quarters according to equation on paper
    forecast_q <- c()
    for (q in 1:4) {
      forecast_q[q] <- (1/(1-phi)) * (4 - (phi * (1-phi^4))/(1-phi)) * mu + ((phi * (1-phi^4))/(1-phi)) * train_series[nrow(train_series) - (q-1),] + (psi * (1-phi^4))/(1-phi) * residuals[nrow(train_series) - (q-1)]
    }
    
    # just making sure
    forecast_values <- as.numeric(forecast_q)
    # Store forecast values
    forecasts[((y - initial_end)*window_size + 1):((y - initial_end+1)*window_size)] <- forecast_values
  }
  
  # decide whether to return forecasts or root mean squared errors (RMSE)
  if (return_rmse) {
    # get data to validate forecasts
    test_series <- data %>% filter(group > initial_end) %>% .[series_name]
    rmse <- sqrt(sum((test_series - forecasts)^2, na.rm=T))
    return(rmse)
  } else{
    return(forecasts)
  }
}

# very similar to the last function, 
# with the difference that now we'll choose AR(p) recursively by SIC
rolling_forecast_ar_p <- function(data, series_name, initial_end=1985, final_date=2002, window_size=4, return_rmse=T) {
  # creating a forecast vector to store results
  forecasts <- numeric((final_date - initial_end)*window_size)
  
  # loop for the rolling window
  for (y in initial_end:(final_date-1)) {
    train_data <- data %>% filter(group <= y)  # Use data up to the current index for training
    train_series <- train_data %>% select(series_name)
    
    # find best ar order
    ar_order <- best_arma_order(train_series)
    # Fit AR(p) model
    arma_model <- stats::arima(train_series, order = c(ar_order, 0, 0))
    
    # Forecast for the next 4 quarters
    #  ... a little note: here we abstract from the way the paper is doing, since it would imply unecessary complexity (look at the equation for the quarterly forecast in page 1174)
    forecast_values <- as.numeric(forecast(arma_model, h = 4)$mean)
    # Store forecast values
    forecasts[((y - initial_end)*window_size + 1):((y - initial_end+1)*window_size)] <- forecast_values
  }
  
  # decide whether to return forecasts or root mean squared errors (RMSE)
  if (return_rmse) {
    # get data to validate forecasts
    test_series <- data %>% filter(group > initial_end) %>% .[series_name]
    rmse <- sqrt(sum((test_series - forecasts)^2, na.rm=T))
    return(rmse)
  } else{
    return(forecasts)
  }
}

# to find the best arma model based on some criteria (still don't contemplate ma orders or other info_criteria than BIC/SIC)
best_arma_order <- function(series, ar_until = 10, ma_order = 0, info_criteria = 'bic'){
  arma_models <- list()
  bic_arma <- c()
  for (p in 1:ar_until) {
    arma_models[[p]] <- arima(series, order = c(p, 0, ma_order))
    # BIC = SIC
    bic_arma[p] <- BIC(arma_models[[p]])
  }
  arma_best_order <- which(min(bic_arma) == bic_arma)
  return(arma_best_order)
}

# running the functions
inflation_series <- df_inflation_authors %>% select(starts_with('inflation')) %>% colnames()

# loops

# defining vectors to store data
arma11_rmse1985 <- c()
arp_rmse <- c()
arma11_forecasts1985 <- list()

# 1985
for (x in 1:length(inflation_series)) {
  print(inflation_series[x])
  # first the simple rmse values for each inflation series
  arma11_rmse1985[x] <- rolling_forecast_arma(df_inflation_authors, inflation_series[x])
  arp_rmse[x] <- rolling_forecast_ar_p(df_inflation_authors, inflation_series[x])
  
  # we need also the forecasts of the arma11 model to run the lambda models
  arma11_forecasts1985[[inflation_series[x]]] <- rolling_forecast_arma(df_inflation_authors, inflation_series[x], return_rmse = F)
}

# printing info
arma11_rmse <- arma11_rmse1985
#
# a loop to get values in the way the table is constructed in the paper
just_rmse <- c()
relative_rmse <- c()
for (x in 1:length(arma11_rmse)) {
  just_rmse <- c(just_rmse, arma11_rmse[x], arp_rmse[x])
  relative_rmse_aux <- c(arma11_rmse[x], arp_rmse[x])
  relative_rmse <- c(relative_rmse, relative_rmse_aux)
}
values_estats <- list(just_rmse, relative_rmse)
table_ts <- create_table_inflation_models(values_estats, inflation_series, c('ARMA', 'AR'), c('RMSE', 'ARMA=1'))
print(table_ts)

# ---
# again for a new year
# ---

# todo: add option to pick the values 

arma11_rmse1995 <- c()
arp_rmse <- c()
arma11_forecasts1995 <- list()

# 1995
for (x in 1:length(inflation_series)) {
  print(inflation_series[x])
  # first the simple rmse values for each inflation series
  arma11_rmse1995[x] <- rolling_forecast_arma(df_inflation_authors, inflation_series[x], initial_end = 1995)
  arp_rmse[x] <- rolling_forecast_ar_p(df_inflation_authors, inflation_series[x])
  
  # we need also the forecasts of the arma11 model to run the lambda models
  arma11_forecasts1995[[inflation_series[x]]] <- rolling_forecast_arma(df_inflation_authors, inflation_series[x], return_rmse = F, initial_end = 1995)
}

# printing info
arma11_rmse <- arma11_rmse1995
#
# a loop to get values in the way the table is constructed in the paper
just_rmse <- c()
relative_rmse <- c()
for (x in 1:length(arma11_rmse)) {
  just_rmse <- c(just_rmse, arma11_rmse[x], arp_rmse[x])
  relative_rmse_aux <- c(arma11_rmse[x], arp_rmse[x])
  relative_rmse <- c(relative_rmse, relative_rmse_aux)
}
values_estats <- list(just_rmse, relative_rmse)
table_ts_1995 <- create_table_inflation_models(values_estats, inflation_series, c('ARMA', 'AR'), c('RMSE', 'ARMA=1'))
print(table_ts_1995)

```

### Phillips Curve (OLS) + Time Structure models


Here, we do a similar procedure as before for the time-series models, however, now we estimate the models using a simple OLS regression. As for the AR(p) model we'll dynamically choose the order of the regressors such that it minimizes the bayesian information criteria (BIC or SIC). Basically, the model we'll estimate goes as follows (for the training sample):

$$
\pi_{t+4,4} = \alpha + \beta'(L) X_t + \epsilon_{t+4,4}
$$
where $\pi_{t+4,4}$ is defined (annually) as previously, $\beta(L)$ is a filter for the variables in $X_t$ that includes the dependent variable (inflation) and another independent variable (some real activity measure, $y_{t+4,4}$). That means that if we have $L=2$, $\beta'(L) X_t$ will include $\pi_{t+3,4}$, $\pi_{t+2,4}$, $y_{t+3,4}$ and $y_{t+2,4}$.

Regarding the window of training and testing, we weren't able to identify whether the authors keep, as before, a rolling window or whether they now use a fixed one. Despite it would make sense to keep the previous approach, we choose to use a fixed window to keep with a conservative/simplicity take.

Moreover, since the process is basically the same as before for the term structure models, we just add them below, then printing the tables.

As previously, we first do for the 1985 period, then for 1995.

```{r phillips_term-structure}
# ---
# Load data ----
# ---

# this value will come from other part of the markdown
# arma11_rmse <- c(0.8938677, 0.8336303, 0.9984834, 0.8883241)

# df_inflation_complete <- read.csv("df_inflation_complete.csv")
df_inflation_authors <- read.csv("data/output/df_inflation_authors.csv")
df_realmeasures_authors <- read.csv("data/output/df_realmeasures_authors.csv")

# Prepare data
df_phillips <- df_inflation_authors %>%
  left_join(df_realmeasures_authors) %>% 
  tibble() %>%
  arrange(FirstDate)

# ---
# Functions ----
# ---

# a function just to lag our dataset
lag_df <- function(df, y, x, lag_until = 5) {
  models <- list()
  bic_models <- c()
  df_lag <- df
  
  # first we loop for the y variable
  for (p in 1:lag_until) {
    df_lag <- df_lag %>% 
      mutate(!!str_c('lag', p, '_', y) := lag(!!sym(y), p))
  }
  
  # then we check whether x is a vector and do the adequate operation 
  if (is.character(x) && length(x) == 1) {
    # If x is a single string, create lagged column for it
    for (p in 1:lag_until) {
      df_lag <- df_lag %>% 
        mutate(!!str_c('lag', p, '_', x) := lag(!!sym(x), p))
    }
  } else if (is.character(x) && length(x) > 1) {
    # If x is a character vector, create lagged columns for each element
    for (var in x) {
      for (p in 1:lag_until) {
        df_lag <- df_lag %>% 
          mutate(!!str_c('lag', p, '_', var) := lag(!!sym(var), p))
      }
    }
  }
  
  return(df_lag)
}

# a function to choose the best lag of the ols based on bic
best_lag_ols <- function(df_lag, y, lag_until=5, criteria='bic'){
  # defining relevant variables
  dependent_var <- y
  independent_var <- c()
  
  # variables to store
  models <- list()
  bic_models <- c()
  
  # loop of lags
  for (p in 1:lag_until) {
    independent_var_plus <- df_lag %>% select(starts_with(paste0('lag', p))) %>% colnames()
    independent_var <- c(independent_var, independent_var_plus)
    reg_formula <- formula(paste(dependent_var, '~', paste(independent_var, collapse=' + ')))
    models[[p]] <- lm(reg_formula, data=df_lag)
    bic_models[p] <- BIC(models[[p]])
  }
  
  # then find the best order
  best_order <- which(min(bic_models) == bic_models)
  
  return(models[[best_order]])
}

# todo: use a rolling window?
# a function to predict for a ols model
ols_predict <- function(train_data, test_data, y, x, max_lag=5){
  # choose the best lag and returns the model
  model <- best_lag_ols(train_data, y, lag_until = max_lag)
  
  # forecast values
  forecast_values <- forecast(model, test_data)
  
  return(forecast_values$mean)
}

# creates a matrix with (y*x) rows and (estats) columns
create_table_inflation_models <- function(values_estats, y, x, estats=c('relative_rmse', '1-lambda', 'hh_error', 'nw_error')){
  # values_stats should be a list of same size as the estats, each value being y*x size (in that order)
  # storing data for table
  table_paper <- matrix(NA, length(y)*length(x), length(estats))  # just for a single estimation period
  # prettify
  colnames(table_paper) <- estats
  names_inflation <- rep(y, each=length(x))
  names_models <- rep(x, length(y))
  rownames(table_paper) <- paste(names_inflation, names_models, sep=' | ')
  
  # adding values
  for (p in 1:length(estats)) {
    table_paper[,p] <- values_estats[[p]]
  }
  
  return(table_paper)
}

# ---
# Run ----
# ---

# change here to change periods (either 1985 or 1995 yet)
initial_estimation_period <- 1985
arma11_rmse <- arma11_rmse1985
arma11_forecasts <- arma11_forecasts1985

# ---
## Phillips models ----
# ---

all_y <- df_phillips %>% select(starts_with('inflation')) %>% colnames()  # "inflation_punew" "inflation_puxhs" "inflation_puxx"  "inflation_pce"
x_phillips <- df_phillips %>% select(gdpg:fac) %>% colnames()  # "gdpg"            "gap1"            "gap2"            "unrate"          "lshr"            "xli"             "xli2"            "fac"

# todo: we need more one loop for the other group: 1995

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
for (y in all_y) {
  print(y)
  for (x in x_phillips) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts[["inflation_punew"]] + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_phillips))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_ols <- create_table_inflation_models(values_estats, all_y, x_phillips)


# ---
# now do it for models with double x's
# ---
doublex_phillips <- list(c('gap1', 'lshr'), c('gap2', 'lshr'))
x_names <- sapply(doublex_phillips, function(x) paste(x, collapse=' + '))
x_now <- doublex_phillips
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
# loop
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts[["inflation_punew"]] + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_ols_2x <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_ols_2x)

# ---
## Term Structure ----
# ---

# basically the same as before, plus rate

all_y <- df_phillips %>% select(starts_with('inflation')) %>% colnames()  # "inflation_punew" "inflation_puxhs" "inflation_puxx"  "inflation_pce"
x_phillips <- df_phillips %>% select(gdpg:fac) %>% colnames()  # "gdpg"            "gap1"            "gap2"            "unrate"          "lshr"            "xli"             "xli2"            "fac"
x_with_rate <- lapply(x_phillips, function(x) c(x, "rate"))
x_names <- sapply(x_with_rate, function(x) paste(x, collapse=' + '))
x_now <- x_with_rate

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts[["inflation_punew"]] + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_term_structure <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_term_structure)

# ---
# now do it for some other specific
# ---
term_structure_x_specific <- list(c('yield'), c('rate', 'yield'), c('gdpg', 'rate', 'yield'))
x_names <- sapply(term_structure_x_specific, function(x) paste(x, collapse=' + '))
x_now <- term_structure_x_specific
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
# loop
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts[["inflation_punew"]] + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_term_structure_specific <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_term_structure_specific)

```

Now for 1995.

```{r phillips_term-structure}

# ---
# Run ----
# ---

# change here to change periods (either 1985 or 1995 yet)
initial_estimation_period <- 1995
arma11_rmse <- arma11_rmse1995
arma11_forecasts <- arma11_forecasts1995

# ---
## Phillips models ----
# ---

all_y <- df_phillips %>% select(starts_with('inflation')) %>% colnames()  # "inflation_punew" "inflation_puxhs" "inflation_puxx"  "inflation_pce"
x_phillips <- df_phillips %>% select(gdpg:fac) %>% colnames()  # "gdpg"            "gap1"            "gap2"            "unrate"          "lshr"            "xli"             "xli2"            "fac"

# todo: we need more one loop for the other group: 1995

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
for (y in all_y) {
  print(y)
  for (x in x_phillips) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts[["inflation_punew"]] + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_phillips))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_ols <- create_table_inflation_models(values_estats, all_y, x_phillips)


# ---
# now do it for models with double x's
# ---
doublex_phillips <- list(c('gap1', 'lshr'), c('gap2', 'lshr'))
x_names <- sapply(doublex_phillips, function(x) paste(x, collapse=' + '))
x_now <- doublex_phillips
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
# loop
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts[["inflation_punew"]] + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_ols_2x <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_ols_2x)

# ---
## Term Structure ----
# ---

# basically the same as before, plus rate

all_y <- df_phillips %>% select(starts_with('inflation')) %>% colnames()  # "inflation_punew" "inflation_puxhs" "inflation_puxx"  "inflation_pce"
x_phillips <- df_phillips %>% select(gdpg:fac) %>% colnames()  # "gdpg"            "gap1"            "gap2"            "unrate"          "lshr"            "xli"             "xli2"            "fac"
x_with_rate <- lapply(x_phillips, function(x) c(x, "rate"))
x_names <- sapply(x_with_rate, function(x) paste(x, collapse=' + '))
x_now <- x_with_rate

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts[["inflation_punew"]] + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_term_structure <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_term_structure)

# ---
# now do it for some other specific
# ---
term_structure_x_specific <- list(c('yield'), c('rate', 'yield'), c('gdpg', 'rate', 'yield'))
x_names <- sapply(term_structure_x_specific, function(x) paste(x, collapse=' + '))
x_now <- term_structure_x_specific
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
# loop
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts[["inflation_punew"]] + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_term_structure_specific <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_term_structure_specific)

```


## Brazilian data

Describe data processing


Repeat all models for Brazilian data.

### Time-Series Models

```{r ts_brazil}

```


### OLS: Phillips Curve + Term Structure Models

```{r phillips_term-structure_brazil}

```

## Critical part

First of all, it would be nice to have a replication package, since this would make verifiability easier (and avoid us the trouble). This is particularly troublesome since the whole point of the paper is to be quantitative, that is, making the best prediction.

However, on top of that, the paper is frequently confusing in terms of the temporal choices for their variables. Probably trying to get more data by using a quaterly frequency, they made the paper more confusing then it would be so by using estimates on an annual basis. Nevertheless, probably in daily forecasting activities data forecast are done more on a quarterly basis, which gives practitioners good advice as how to perform it.

Finally, seasonality ...

Why in the real activity measures we don't let it affect contemporaneously.
