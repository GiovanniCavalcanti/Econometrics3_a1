---
title: "Replication Econometrics III -- A1"
author: "Calebe Piacentini and Giovanni Cavalcanti"
date: "2024-01-25"
output: html_document
---

All the code available to this project can be found in the github link below:

[https://github.com/GiovanniCavalcanti/Econometrics3_a1]

## What do we need to replicate?

As exposed in the guidelines, we need to replicate all tables and figures **except** for the parts involving models RGM, VAR, RGMVAR, MDL1 and MDL2.

Altering table 2 from the paper to exclude these models, we have:

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dplyr)
library(quantmod)
library(forecast)
library(lubridate)
library(gt)
library(tibble)
library(knitr)
library(kableExtra)
library(sandwich)
```


```{r models, echo=FALSE}
# Create the data frame
models_table <- data.frame(
 Description = c(
  "Time-series models", # 1
  "", # 2
  "", # 3
  "", # 4
  "Phillips curve (OLS)", # 5
  "", # 6
  "", # 7
  "", # 8
  "", # 9
  "", # 10
  "", # 11
  "", # 12
  "", # 13
  "", # 14
  "OLS term structure models", # 15
  "", # 16
  "", # 17
  "", # 18
  "", # 19
  "", # 20
  "", # 21
  "", # 22
  "", # 23
  "", # 24
  "", # 25
  "Inflation surveys", # 26
  "", # 27
  "", # 28
  "", # 29
  "", # 30
  "", # 31
  "", # 32
  "", # 33
  "" # 34
 ),
 Abbreviation = c(
  "ARMA", # 1
  "AR", # 2
  "RW", # 3
  "AORW", # 4
  "PC1", # 5
  "PC2", # 6
  "PC3", # 7
  "PC4", # 8
  "PC5", # 9
  "PC6", # 10
  "PC7", # 11
  "PC8", # 12
  "PC9", # 13
  "PC10", # 14
  "TS1", # 15
  "TS2", # 16
  "TS3", # 17
  "TS4", # 18
  "TS5", # 19
  "TS6", # 20
  "TS7", # 21
  "TS8", # 22
  "TS9", # 23
  "TS10", # 24
  "TS11", # 25
  "SPF1", # 26
  "SPF2", # 27
  "SPF3", # 28
  "LIV1", # 29
  "LIV2", # 30
  "LIV3", # 31
  "MICH1", # 32
  "MICH2", # 33
  "MICH3" # 34
 ),
 Specification = c(
  "ARMA(1,1)", # 1
  "Autoregressive model", # 2
  "Random walk on quarterly inflation", # 3
  "Random walk on annual inflation", # 4
  "INFL + GDPG", # 5
  "INFL + GAP1", # 6
  "INFL + GAP2", # 7
  "INFL + LSHR", # 8
  "INFL + UNEMP", # 9
  "INFL + XLI", # 10
  "INFL + XLI-2", # 11
  "INFL + FAC", # 12
  "INFL + GAP1 + LSHR", # 13
  "INFL + GAP2 + LSHR", # 14
  "INFL + GDPG + RATETS", # 15
  "INFL + GAP1 + RATETS", # 16
  "INFL + GAP2 + RATETS", # 17
  "INFL + LSHR + RATETS", # 18
  "INFL + UNEMP + RATETS", # 19
  "INFL + XLI + RATETS", # 20
  "INFL + XLI-2 + RATETS", # 21
  "INFL + FAC + RATETS", # 22
  "INFL + SPDTS", # 23
  "INFL + RATE + SPDTS", # 24
  "INFL + GDPG + RATE + SPDT", # 25
  "VAR(1) on RATE, SPD, INFL, GDPG", # 26
  "Linear bias-corrected SPF", # 27
  "Non-linear bias-corrected SPFLIV1", # 28
  "Livingston survey", # 29
  "Linear bias-corrected Livingston", # 30
  "Non-linear bias-corrected Livingston", # 31
  "Michigan survey", # 32
  "Linear bias-corrected Michigan", # 33
  "Non-linear bias-corrected Michigan" # 34
 )
)

knitr::kable(models_table)
```

First we'll replicate the authors work with similar data and then we'll do the same but for the Brazilian data.

## Loading Data

Data from the paper.

Part of the data (mainly from real data sources and when it comes from other papers) was gathered through the authors data replication kit (asked by e-mail).

There are four different measures of inflation. The first three are consumer prices index (CPI) measures, including

- CPI-U for all urban consumers, all items (PUNEW),
    
- CPI for all urban consumers, all items less shelter (PUXHS)
    
- and CPI for all urban consumers, all items less food and energy (PUXX), which is also called core CPI.

The fourth measure is the personal consumption expenditure deflator (PCE)

All measures are seasonally adjusted and obtained from the **Bureau of Labor Statistics** website.

The sample period is

- 1952:Q2–2002:Q4 for PUNEW and PUXHS,
    
- 1958:Q2–2002:Q4 for PUXX,
    
- 1960:Q2–2002:Q4 for PCE
    

## Real activities measures

Six different measures of real activity along with one composite real activity factor.

- gdpg: GDP growth using seasonally adjusted data on real GDP in billions of chained 2000 dollars.
    
- unemp: Unemployment rate is also seasonally adjusted and computed for the civilian labor force aged 16 years and over
    

Both real GDP and the unemployment rate are from the Federal Reserve economic data (FRED) database.

- gap1: output gap either as the detrended log real GDP by removing a quadratic trend as in Gali and Gertler (1999).
    
- gap2: Hodrick–Prescott (1997) filter (with the standard smoothness parameter of 1,600).
    

Both measures are constructed using only current and past GDP values, so the filters are run recursively.

- lshr: labor income share, defined as the ratio of nominal compensation to total nominal output in the U.S. nonfarm business sector.
    

Two forward-looking indicators:

- li: Stock–Watson (1989) experimental leading index (LI).
    
- xli: “Stock–Watson (1989) alternative nonfinancial experimental leading index-2
    

Because Stock and Watson (2002a), among others, show that aggregating the information from many factors has good forecasting power, we also use a single factor aggregating the information from 65 individual series constructed by Bernanke et al. (2005).

- fac: aggregates real output and income, employment and hours, consumption, housing starts and sales, real inventories, and average hourly earnings.
    

The sample period for all the real activity measures is 1952:Q2–2001:Q4, except the Bernanke–Boivin–Eliasz real activity factor, which spans 1959:Q1–2001:Q3.

## Surveys

We examine three inflation expectation surveys: the Livingston survey, the survey of professional forecasters (SPF), and the Michigan survey.

The Livingston survey is conducted twice a year, in June and in December, and polls economists from industry, government, and academia. The Livingston survey records participants’ forecasts of non-seasonally adjusted CPI levels six and twelve months in the future and is usually conducted in the middle of the month.

We follow Thomas (1999) and Mehra (2002) and adjust the raw Livingston forecasts by a factor of 12/14 to obtain an annual inflation forecast.

The CPI variable is the most requested variable of the Livingston Survey’s data set; it is, therefore, also the most scrutinized.  Several minor inconsistencies characterize this variable.  The problems usually arise because Joseph Livingston requested forecasts for CPI levels rather than inflation rates.  Users should also know that these forecasts are for the not-seasonally adjusted level of the CPI in surveys prior to December 2004. Beginning with the survey of December 2004, we asked the panelists to provide forecasts for the seasonally adjusted index level.  

Participants in the SPF are drawn primarily from business, and forecast changes in the quarterly average of seasonally adjusted CPI-U levels. The SPF is conducted in the middle of every quarter and the sample period for the SPF median forecasts is from 1981:Q3 to 2002:Q4.

The Michigan survey is conducted monthly and asks households, rather than professionals, to estimate expected price changes over the next twelvemonths.We use the median Michigan survey forecast of inflation over the next year at the end of each quarter from 1978:Q1 to 2002:Q4.

Obtain data for the Livingston survey and SPF data from the Philadelphia Fed website (http:// www.phil.frb.org/econ/liv and http://www.phil.frb.org/econ/spf, respectively)

We take the Michigan survey data from the St. Louis Federal Reserve FRED database (http://research.stlouisfed.org/fred2/series/MICH/)

We'll do the following procedure: first we gather data into the environment, save it to new files and, and given the necessity, reload them in each new chunk.

```{r}
# 00 Packages and environment -----------------------

# Limpar o ambiente de trabalho
rm(list = ls()) 

# Carreggar pacotes necessários
library(tidyverse)
library(dplyr)
library(quantmod)
library(forecast)
library(lubridate)
library(gt)
library(tibble)
library(knitr)
library(kableExtra)
library(readxl)

# 01 Load and adjust inflation data -----------------------

# function to process inflation data
# file_path = file_path, ex: "~/data/punew_1947-2023.csv"
# inflation_label = series_name, ex: "inflation_punew"

process_inflation_data <- function(file_path, inflation_label, period_column = "Period", label_column = "Label", value_column = "Value") {
  df <- read_csv(file_path) %>%
    select(Period = period_column, Label = label_column, P = value_column) %>%
    mutate(
      DATE = dmy(paste("01", substr(Period, 2, 3), substr(Label, 1, 4), sep = "-")), # Generate DATE column early to use for arrangement
      P_lag = lag(P, n =3), # Create a lagged version of the price column
      !!inflation_label := log(P / P_lag) # Dynamically name the inflation column
    ) %>%
    na.omit() %>%
    arrange(DATE) %>%
    mutate(Quarter = paste(year(DATE), quarter(DATE), sep = "-Q")) %>%
    group_by(Quarter) %>%
    summarise(
      FirstDate = first(DATE),
      FirstP = first(P),
      FirstP_lag = first(P_lag),
      !!inflation_label := first(!!sym(inflation_label))
    ) %>%
    select(Quarter, FirstDate, FirstP, FirstP_lag, !!inflation_label)
}

# Process each dataset
punew_df <- process_inflation_data("data/input/inflation_data/punew_1947-2023.csv", "inflation_punew")
puxhs_df <- process_inflation_data("data/input/inflation_data/puxhs_1947-2023.csv", "inflation_puxhs")
puxx_df <- process_inflation_data("data/input/inflation_data/puxx_1957-2023.csv", "inflation_puxx")

rm(process_inflation_data)

pce_df <- read_csv("data/input/inflation_data/pce_DPCERD3Q086SBEA_1947-2023.csv") %>%
  rename(P = "DPCERD3Q086SBEA") %>%
  mutate(
    P_lag = lag(P),
    inflation_pce = log(P / P_lag),
    FirstDate = DATE
  ) %>%
  na.omit()


# Merge all dataframes on the FirstDate column
df_inflation_complete <- reduce(list(punew_df, puxhs_df, puxx_df, pce_df), full_join, by = "FirstDate") %>%
  select("FirstDate", contains("inflation")) %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  mutate(group = year(FirstDate))

# calebe: thank you for noting this!
##___________________________________________________________________##
#   Save the df_inflation_complete as df_inflation_complete.csv.      
#   This is the the complete inflation panel!                           
##___________________________________________________________________##
# write.csv(df_inflation_complete, file = "df_inflation_complete.csv")

rm(pce_df, punew_df, puxhs_df, puxx_df)

# Now, we aggregate inflation by year.

punew_year <- df_inflation_complete %>%
  mutate(punew_year = inflation_punew + lag(inflation_punew, 1) +
           lag(inflation_punew, 2) + lag(inflation_punew, 3)) %>%
  select(!contains("inflation"))
puxhs_year <- df_inflation_complete %>%
  mutate(puxhs_year = inflation_puxhs + lag(inflation_puxhs, 1) +
           lag(inflation_puxhs, 2) + lag(inflation_puxhs, 3)) %>%
  select(!contains("inflation"))
puxx_year <- df_inflation_complete %>%
  mutate(puxx_year = inflation_puxx + lag(inflation_puxx, 1) +
           lag(inflation_puxx, 2) + lag(inflation_puxx, 3)) %>%
  select(!contains("inflation"))
pce_year <- df_inflation_complete %>%
  mutate(pce_year = inflation_pce + lag(inflation_pce, 1) +
           lag(inflation_pce, 2) + lag(inflation_pce, 3)) %>%
  select(!contains("inflation"))

df_inflation_complete_yearly <- full_join(punew_year, puxhs_year ) %>%
  full_join(., puxx_year ) %>%
  full_join(., pce_year ) 

##____________________________________________________________________________##
#   df_inflation_complete_yearly is the yearly aggregated inflation     
#   This is the complete yearly inflation panel  
##____________________________________________________________________________##
# write.csv(df_inflation_complete_yearly, file = "df_inflation_complete_yearly.csv")

rm(pce_year, punew_year, puxhs_year, puxx_year)

## 02.0 Recreate Authors' inflation panel -------------------------------------------

##______________________________________________________________________________##
# To recreate Authors' inflation panel, it requires to filter the dates accordingly
# Their sample period is:
#- 1952:Q2–2002:Q4 for PUNEW and PUXHS,
#- 1958:Q2–2002:Q4 for PUXX,
#- 1960:Q2–2002:Q4 for PCE
##______________________________________________________________________________##

# Filter the data based on the specified date ranges for each series
punew_puxhs_filter <- filter(df_inflation_complete, FirstDate >= as.Date("1952-04-01") & FirstDate <= as.Date("2002-10-01")) %>%
  select("FirstDate", "inflation_punew", "inflation_puxhs", "quarter")
puxx_filter <- filter(df_inflation_complete, FirstDate >= as.Date("1958-04-01") & FirstDate <= as.Date("2002-10-01")) %>%
  select("FirstDate", "inflation_puxx", "quarter")
pce_filter <- filter(df_inflation_complete, FirstDate >= as.Date("1960-04-01") & FirstDate <= as.Date("2002-10-01")) %>%
  select("FirstDate", "inflation_pce", "quarter")

# Now, join those dataframes
df_inflation_authors <- full_join(punew_puxhs_filter, puxx_filter, by = "FirstDate") %>%
  full_join(., pce_filter, by = "FirstDate")  %>%
  mutate(group = year(FirstDate)) %>%
  select("FirstDate", "inflation_punew", "inflation_puxhs", "inflation_puxx", "inflation_pce", "quarter" = "quarter.x", "group") 

##____________________________________________________________________________##
#   Save the df_inflation_authors as original_inflation_panel.csv.  
#   This is the the authors' original inflation panel        
##____________________________________________________________________________##
# write.csv(df_inflation_authors, file = "df_inflation_authors.csv")

# Now, we agregate inflation by year.

punew_year <- df_inflation_authors %>%
  mutate(punew_year = inflation_punew + lag(inflation_punew, 1) +
           lag(inflation_punew, 2) + lag(inflation_punew, 3)) %>%
  select(!contains("inflation"))
puxhs_year <- df_inflation_complete %>%
  mutate(puxhs_year = inflation_puxhs + lag(inflation_puxhs, 1) +
           lag(inflation_puxhs, 2) + lag(inflation_puxhs, 3)) %>%
  select(!contains("inflation"))
puxx_year <- df_inflation_complete %>%
  mutate(puxx_year = inflation_puxx + lag(inflation_puxx, 1) +
           lag(inflation_puxx, 2) + lag(inflation_puxx, 3)) %>%
  select(!contains("inflation"))
pce_year <- df_inflation_complete %>%
  mutate(pce_year = inflation_pce + lag(inflation_pce, 1) +
           lag(inflation_pce, 2) + lag(inflation_pce, 3)) %>%
  select(!contains("inflation"))

df_inflation_authors_yearly <- full_join(punew_year, puxhs_year ) %>%
  full_join(., puxx_year ) %>%
  full_join(., pce_year ) 
##____________________________________________________________________________##
#   df_inflation_authors_yearly is the yearly aggregated inflation     
#   This is the the authors' original yearly inflation panel  
##____________________________________________________________________________##
# write.csv(df_inflation_authors_yearly, file = "df_inflation_authors_yearly.csv")

rm(pce_year, punew_year, puxhs_year, puxx_year)


# 02 Load and adjust real activities measures ----------------------------------

## gdpg dataframe
df_gdp_complete <- read_csv("data/input/real_measures_data/gdp.csv") %>%
  select("FirstDate" = "DATE", "gdp" = "GDPC1") %>%
  mutate(gdp_lag = lag(gdp, n=1), 
         gdpg = log(gdp / gdp_lag), 
         group = year(FirstDate),
         FirstDate = as.Date(FirstDate)) %>%
  select("FirstDate", "gdpg", "group") %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  na.omit()

## gap1 dataframe ((quadratic) detrended log GDP as a measure of the output gap last period)
df_gap1_complete <- read.csv("data/input/real_measures_data/gdp.csv") %>%
  select("FirstDate" = "DATE", "gdp" = "GDPC1") %>%
  mutate(FirstDate = as.Date(FirstDate)) %>%
  mutate(quarter = as.yearqtr(FirstDate), 
         gap1 = log(lag(gdp, n =1))^2,
         group = year(FirstDate),
         FirstDate = as.Date(FirstDate)) %>%
  select("FirstDate", "gap1", "quarter", "group") %>%
  na.omit()

## gap2 dataframe

library(hpfilter) # implements the modified filter for gap2

gap2 <- read_csv("data/input/real_measures_data/gdp.csv") %>%
  select("gdp" = "GDPC1") 

filter <- hp1(gap2, lambda = 1600)

gap2 <- read.csv("data/input/real_measures_data/gdp.csv") %>%
  select("FirstDate" = "DATE", "gdp" = "GDPC1")

gap2$gap_2 <- filter

df_gap2_complete <- gap2 %>%
  select("FirstDate" = as.character("FirstDate"), "gap2" = "gap_2") %>%
  unnest(gap2) %>%
  mutate(FirstDate = as.Date(FirstDate),
         gap2 = gdp,
         quarter = as.yearqtr(FirstDate),
         group = year(FirstDate)) %>%
  select(!gdp)

rm(gap2, filter)

## li/lix dataframes

a <- readLines("data/input/real_measures_data/experimental_leading_indexes/xindex.ASC", skip = 10)
a <- a[11:550]


str_split(a[50], "\\s+")

year <- c()

for (x in 1:length(a)) {
  line_split <- str_split(a[x], "\\s+")
  
  year[x] <- line_split[[1]][2]
}

xli <- c()

for (x in 1:length(a)) {
  line_split <- str_split(a[x], "\\s+")
  
  xli[x] <- line_split[[1]][4]
}

xli2 <- c()

for (x in 1:length(a)) {
  line_split <- str_split(a[x], "\\s+")
  
  xli2[x] <- line_split[[1]][8]
}

teste <- data.frame(year, xli, xli2)

df_experimentalindex_complete <- teste[-1,]

df_experimentalindex_complete <- df_experimentalindex_complete %>% 
  mutate(FirstDate = dmy(paste("01", substr(year, 6, 7), substr(year, 1, 4), sep = "-")),
         xli = as.numeric(xli),
         xli2 = as.numeric(xli2)) %>%
  mutate(quarter = as.yearqtr(FirstDate),
         group = year(FirstDate)) %>%
  select(FirstDate, xli, xli2, quarter, group) %>%
  group_by(quarter) %>%
  summarise(
    FirstDate = first(FirstDate), 
    xli = first(xli),
    xli2 = first(xli2),
    group = first(group)
    )

rm(teste, line_split, a, x)

## fac dataframe

a <- readLines("data/input/real_measures_data/factor.txt")
a <- a[3:513]

factor <- c()

for (x in 1:length(a)) {
  line_split <- str_split(a[x], "\\s+")
  
  factor[x] <- line_split[[1]][2]
}

teste <- data.frame(as.numeric(factor)) 

# Generate the sequence of dates by month
date_sequence <- seq.Date(from = as.Date("1959-02-01"), to = as.Date("2001-08-01"), by = "month")

# Create a data frame with this date sequence as a column
df_months <- data.frame(FirstDate = date_sequence)

df_fac_complete <- bind_cols(teste, df_months) %>%
  mutate(fac = as.numeric.factor.) %>%
  mutate(quarter = as.yearqtr(FirstDate),
         group = year(FirstDate)) %>%
  select(FirstDate, fac, quarter, group) %>%
  group_by(quarter) %>%
  summarise(
    FirstDate = first(FirstDate), 
    fac = first(fac),
    group = first(group)
  )

## unemployment  dataframe
df_unemp_complete <- read.csv("data/input/real_measures_data/unemp.csv") %>%
  arrange(DATE) %>%
  mutate(DATE = as.Date(DATE)) %>%
  mutate(Quarter = as.yearqtr(DATE)) %>%
  group_by(Quarter) %>%
  summarise(
    FirstDate = first(DATE),
    UNRATE = first(UNRATE)) %>%
  mutate(FirstDate = as.Date(FirstDate),
         group = year(FirstDate)) %>%
  select("FirstDate", "unrate" = "UNRATE", "quarter" = "Quarter", "group")

## labor share dataframe
df_lshr_complete <- read.csv("data/input/real_measures_data/lshr.csv") %>%
  select("FirstDate" = "DATE", "lshr" = "PRS85006173") %>%
  mutate(group = year(FirstDate),
         FirstDate = as.Date(FirstDate)) %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  select("FirstDate", "lshr", "quarter", "group")

rm(x, xli, xli2, year, a, date_sequence, teste, df_months, factor, line_split)

##fb_rate dataframe 

a <- readLines("data/input/real_measures_data/FBrate.txt")
a <- a[3:609]

factor <- c()

for (x in 1:length(a)) {
  line_split <- str_split(a[x], "\\s+")
  
  factor[x] <- line_split[[1]][2]
}

teste <- data.frame(as.numeric(factor)) 

# Generate the sequence of dates by month
date_sequence <- seq.Date(from = as.Date("1952-07-01"), to = as.Date("2003-01-01"), by = "month")

# Create a data frame with this date sequence as a column
df_months <- data.frame(FirstDate = date_sequence)

df_rate_complete <- bind_cols(teste, df_months) %>%
  mutate(rate = as.numeric.factor.) %>%
  mutate(quarter = as.yearqtr(FirstDate),
         group = year(FirstDate)) %>%
  select(FirstDate, rate, quarter, group) %>%
  group_by(quarter) %>%
  summarise(
    FirstDate = first(FirstDate), 
    rate = first(rate),
    group = first(group)
  )

##fb_yield dataframe 

a <- readLines("data/input/real_measures_data/FByield.txt")
a <- a[3:609]

factor <- c()

for (x in 1:length(a)) {
  line_split <- str_split(a[x], "\\s+")
  
  factor[x] <- line_split[[1]][2]
}

teste <- data.frame(as.numeric(factor)) 

# Generate the sequence of dates by month
date_sequence <- seq.Date(from = as.Date("1952-07-01"), to = as.Date("2003-01-01"), by = "month")

# Create a data frame with this date sequence as a column
df_months <- data.frame(FirstDate = date_sequence)

df_yield_complete <- bind_cols(teste, df_months) %>%
  mutate(yield = as.numeric.factor.) %>%
  mutate(quarter = as.yearqtr(FirstDate),
         group = year(FirstDate)) %>%
  select(FirstDate, yield, quarter, group) %>%
  group_by(quarter) %>%
  summarise(
    FirstDate = first(FirstDate), 
    yield = first(yield),
    group = first(group)
  )

## join all dataframees
df_realmeasures_complete <- full_join(df_gdp_complete, df_unemp_complete, ) %>%
  full_join(., df_lshr_complete) %>%
  full_join(., df_gap1_complete) %>%
  full_join(., df_gap2_complete) %>%
  full_join(., df_experimentalindex_complete) %>%
  full_join(., df_fac_complete) %>%
  full_join(., df_rate_complete) %>%
  full_join(., df_yield_complete) %>%
  select("FirstDate", "gdpg", "gap1", "gap2", "unrate", "lshr", "xli", "xli2", "fac", "rate", "yield", "quarter", "group")

df_realmeasures_complete <- df_realmeasures_complete[1:(nrow(df_realmeasures_complete) - 2),]

##____________________________________________________________________________##
#   df_realmeasures_complete is the quarterly real measures data    
#   This is the complete quarterly real measures data 
##____________________________________________________________________________##
# write.csv(df_realmeasures_complete, file = "df_realmeasures_complete.csv")

rm(df_gdp_complete, df_unemp_complete, df_lshr_complete, df_gap1_complete, df_gap2_complete)

## 0.3.0 Recreate Authors' Real measures panel

##______________________________________________________________________________##
# To recreate Authors' real measures panel, it requires to filter the dates accordingly
# Their sample period is:
#- 1952:Q2–2001:Q4 for all the real activity measures, except
#- 1959:Q1–2001:Q3 for Bernanke–Boivin–Eliasz real activity factor
##______________________________________________________________________________##

df_realmeasures_authors <- filter(df_realmeasures_complete, FirstDate >= as.Date("1952-04-01") & FirstDate <= as.Date("2001-10-01")) 

##____________________________________________________________________________##
#   df_realmeasures_authors is the quarterly real measures data    
#   This is the Authors' quarterly real measures data 
##____________________________________________________________________________##
# write.csv(df_realmeasures_authors, file = "df_realmeasures_authors.csv")

# 03 Load and adjust survey measures -------------------------------------

# Load survey data from Excel, adjust CPI calculations, and resample bi-quarterly
df_livingston_complete <- read_excel('data/input/survey_data/livingston_survey.xlsx', sheet = 'CPI', na = "#N/A") %>%
  mutate(
    liv_year = if_else(is.na(CPI_12M), NA_real_, (log(CPI_12M) - log(CPI_BP)) * 100),
    Date = as.Date(Date),
    quarter = as.yearqtr(Date),
    group = year(Date)
  ) %>%
  select(quarter, group, liv_year)

# Load and adjust SPF survey data
df_spf_complete <- read_excel("data/input/survey_data/spf_survey.xlsx", na = "#N/A") %>%
  select(year = YEAR, quarter = QUARTER, CPIB) %>%
  unite("quarter", year:quarter, sep = " Q") %>%
  mutate(quarter = as.yearqtr(quarter)) %>%
  rename(spf_year = CPIB)

# Load and adjust Michigan survey data, resample annually
df_michigan_complete <- read.csv("data/input/survey_data/michigan_survey_inflation.csv") %>%
  rename(date = DATE) %>%
  mutate(
    date = as.Date(date),
    quarter = as.yearqtr(date)) %>%
  group_by(quarter = as.yearqtr(date - months(12))) %>% # Shift by 4 quarters (1 year) directly
  summarise(mich_year = mean(MICH, na.rm = TRUE), .groups = 'drop')

df_surveys_complete <- full_join(df_livingston_complete, df_michigan_complete) %>%
  full_join(df_spf_complete) %>%
  mutate(group = year(quarter)) %>%
  arrange(quarter)

# Save all created dataframes as csv
write.csv(df_experimentalindex_complete, file = "data/output/df_experimentalindex_complete.csv")
write.csv(df_fac_complete, file = "data/output/df_fac_complete.csv")
write.csv(df_inflation_authors, file = "data/output/df_inflation_authors.csv")
write.csv(df_inflation_authors_yearly, file = "data/output/df_inflation_authors_yearly.csv")
write.csv(df_inflation_complete, file = "data/output/df_inflation_complete.csv")
write.csv(df_inflation_complete_yearly, file = "data/output/df_inflation_complete_yearly.csv")
write.csv(df_livingston_complete, file = "data/output/df_livingston_complete.csv")
write.csv(df_michigan_complete, file = "data/output/df_michigan_complete.csv")
write.csv(df_realmeasures_authors, file = "data/output/df_realmeasures_authors.csv")
write.csv(df_experimentalindex_complete, file = "data/output/df_experimentalindex_complete.csv")
write.csv(df_realmeasures_complete, file = "data/output/df_realmeasures_complete.csv")
write.csv(df_spf_complete, file = "data/output/df_spf_complete.csv")
write.csv(df_surveys_complete, file = "data/output/df_surveys_complete.csv")
```

And the same for Brazilian data.

```{r}
# 01 Environment and packages --------------------------------------------------
# Clear the workspace to ensure a clean environment for the analysis.

# Load necessary R packages for data manipulation, visualization, and analysis.
# These libraries provide a wide range of functions for data processing, financial modeling,
# time series forecasting, working with dates, creating tables, dynamic reporting, and more.
library(tidyverse)    # Collection of data science packages, including dplyr for data manipulation and ggplot2 for plotting.
library(dplyr)        # Data manipulation tools for filtering, selecting, and transforming data.
library(quantmod)     # Tools for quantitative financial modeling and trading strategy development.
library(forecast)     # Functions for forecasting time series data.
library(lubridate)    # Simplifies working with dates and times.
library(gt)           # Enables creation of beautiful and customizable tables.
library(tibble)       # Provides a modern reimagining of data frames.
library(knitr)        # Allows for dynamic report generation in R.
library(kableExtra)   # Enhances knitr::kable() outputs with additional styling and functionality.
library(readxl)       # Enables reading Excel files directly into R.
library(GetBCBData)   # Fetches data from the Brazilian Central Bank (BCB).
library(rbcb)         # Provides access to BCB's web services for economic data.

# 02 Load and adjust inflation data --------------------------------------------

# For retrieving specific series by ID from the Brazilian Central Bank, visit:
# https://www3.bcb.gov.br/sgspub/localizarseries/localizarSeries.do?method=prepararTelaLocalizarSeries
# Use the provided link to locate series by their code numbers.

## Extract inflation data
# Define IDs for the IPCA and EXFE series.
my.id <- c('ipca' = 433, 'ipca_15' = 7478, 'exfe' = 28751)
# Fetch series data within a 30-year range from today.
df <- gbcbd_get_series(my.id, cache.path = tempdir(),
                       first.date = Sys.Date() - 30 * 365,
                       last.date = Sys.Date())
# Process the fetched data into a cleaner format.
df_inflation_brazil <- df %>%
  select(!id.num) %>%
  pivot_wider(names_from = series.name, values_from = value) %>%
  mutate(quarter = as.yearqtr(ref.date), group = year(ref.date)) %>%
  group_by(quarter) %>%
  filter(row_number() == 1) %>%
  ungroup()

# 02 Load and adjust real economic measures ------------------------------------

# Define IDs for GDP growth, two unemployment measures, and labor participation.
my.id <- c('gdp' = 4380, 'unemp_desat' = 1620, 'unemp_pnad' = 24369, "lbr_part" = 28544)
# Fetch and process real measures data similar to inflation data.
df <- gbcbd_get_series(my.id, cache.path = tempdir(),
                       first.date = Sys.Date() - 30 * 365,
                       last.date = Sys.Date())
df_realmeasures_brazil <- df %>%
  select(!id.num) %>%
  pivot_wider(names_from = series.name, values_from = value) %>%
  rename(FirstDate = ref.date)

# Load the dataset, skipping the first row and excluding the second column
desemprego_pme_descontinuado <- read_delim("brazil_real_measures_data/desemprego_pme_descontinuado.csv", 
                                           delim = ";", 
                                           escape_double = FALSE, 
                                           col_names = FALSE, 
                                           trim_ws = TRUE, 
                                           skip = 1) %>%
  select(-X2) # Exclude the second column right after loading

desemprego_pme_descontinuado <-as.data.frame(t(as.matrix(desemprego_pme_descontinuado)), header = TRUE) %>%
  select(V1, V4)

desemprego_pme_descontinuado <- desemprego_pme_descontinuado %>%
  slice(-1) %>% # Remove the first row which is likely a header or unwanted data
  mutate(V4 = as.numeric(V4), # Convert V4 to numeric
         FirstDate = dmy(paste("01", V1)), # Create a FirstDate column assuming the first day of the month
         quarter = as.yearqtr(FirstDate), # Create a quarter column from FirstDate
         group = year(FirstDate)) %>% # Create a group column based on the year of FirstDate
  select(FirstDate, unrate = V4, quarter, group) # Select and rename columns

df_realmeasures_brazil <- df_realmeasures_brazil%>%
  left_join(desemprego_pme_descontinuado) %>%
  mutate(unrate = coalesce(unemp_desat, unemp_pnad, unrate)) %>%
  select(FirstDate, gdp, unrate, lbr_part) %>%
  mutate(quarter = as.yearqtr(FirstDate),
         group = year(FirstDate)) %>%
  mutate(gdp_lag = lag(gdp, n=1), 
         gdpg = log(gdp / gdp_lag)) %>%
  mutate(gap1 = log(lag(gdp, n =1))^2) %>%
  mutate(lshr = lbr_part/gdp)

library(hpfilter) # implements the modified filter for gap2

gap2 <-  df_realmeasures_brazil%>%
  select(gdp)

filter <- hp1(gap2, lambda = 1600)

gap2 <- df_realmeasures_brazil %>%
  select(FirstDate, gdp)

gap2$gap_2 <- filter

df_gap2_complete <- gap2 %>%
  select("FirstDate" = as.character("FirstDate"), "gap2" = "gap_2") %>%
  unnest(gap2) %>%
  mutate(FirstDate = as.Date(FirstDate),
         gap2 = gdp,
         quarter = as.yearqtr(FirstDate),
         group = year(FirstDate)) %>%
  select(!gdp)

rm(gap2, filter)

df_realmeasures_brazil <- df_realmeasures_brazil %>%
  left_join(df_gap2_complete) 

df_realmeasures_brazil <- df_realmeasures_brazil %>%
  arrange(FirstDate) %>%
  select(FirstDate, gdpg, gap1, gap2, unrate, lshr, quarter, group) %>%
  group_by(quarter) %>%
  filter(row_number() == 1) %>%
  ungroup()

# 03 Load and adjust FOCUS surveys ---------------------------------------------

# Define the economic indicators of interest.
indic <- c("IPCA", "IPCA-15", "IPCA Alimentação no domicílio")
# Fetch survey data for the defined indicators.
df_surveys_brazil <- get_annual_market_expectations(indic) %>%
  select(indic, FirstDate = date, reference_date, median) %>%
  pivot_wider(names_from = indic, values_from = median, names_prefix = "indic_", values_fn = mean) %>%
  filter(reference_date == year(FirstDate) + 1) %>%
  filter(month(FirstDate) %in% c(1, 4, 7, 10)) %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  group_by(quarter) %>%
  filter(row_number() == 1) %>%
  ungroup()

# 04 save the results as csv files on the appropriate folder --------------------

write.csv(df_inflation_brazil, file = "brazil_data/df_inflation_brazil.csv")
write.csv(df_realmeasures_brazil, file = "brazil_real_measures_data/df_realmeasures_brazil.csv")
write.csv(df_surveys_brazil, file = "brazil_survey_data/df_surveys_brazil.csv")
```

## Summary statistics

Here we replicate table 1 of Summary Statistics from the paper and the following figures.

First, we do it for the exactly the period of their analysis. It's worth noting that although their first year of analysis is 1952, PUXX only starts to have data at 1957 (even though they use data starting from 1958).

```{r replication_a1_summary-stats}
# missing packages

## 02.1 Recreate table 1 Summary Statistics - original --------------------------

### Panel A --------------------------------------------------

df_inflation_authors <- read_csv('data/output/df_inflation_authors.csv')
df_inflation_authors_yearly <- read_csv('data/output/df_inflation_authors_yearly.csv')
data_variables <- select(df_inflation_authors_yearly, ends_with('year'))

# Calculate summary statistics
means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_a <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# New column to add at the start
statistics_labels <- c("Mean", "Standard deviation", "Autocorrelation", "Correlations", "PUXHS", "PUXX", "PCE")

# Adding the new column at the start of the dataframe
panel_a <- panel_a %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
panel_a <- kable(panel_a, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel A: 1952:Q2–2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

(panel_a)

rm(panel_a, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel B -----------------------------------------------

df_inflation_authors_B <- df_inflation_authors %>%
  filter(FirstDate >= "1986-01-01" & FirstDate <= "2002-10-01")

punew_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

df_inflation_authors_yearly_B <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(df_inflation_authors_yearly_B, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors_B %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_b <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_b <- panel_b %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel B (latex code)
panel_b <- kable(panel_b, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel B: 1986:Q1– 2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

(panel_b)

rm(panel_b, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel C -----------------------------------------------

df_inflation_authors_C <- df_inflation_authors %>%
  filter(FirstDate >= "1996-01-01" & FirstDate <= "2002-10-01")

punew_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

df_inflation_authors_yearly_C <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(df_inflation_authors_yearly_C, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors_C %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_c <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_c <- panel_c %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel C (latex code)
panel_c <- kable(panel_c, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel C: 1996:Q1– 2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

(panel_c)
# Clear environment
rm(panel_c, sds, means, autocorrelation_quaterly, corr_table, data_variables)
rm(df_inflation_authors_B, df_inflation_authors_C, df_inflation_authors_yearly_B, df_inflation_authors_yearly_C, statistics_labels)

## 02.2 Recreate table 1 Summary Statistics - complete --------------------------

### Panel A --------------------------------------------------

data_variables <- select(df_inflation_complete_yearly, ends_with('year'))

# Calculate summary statistics
means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_complete %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_a <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# New column to add at the start
statistics_labels <- c("Mean", "Standard deviation", "Autocorrelation", "Correlations", "PUXHS", "PUXX", "PCE")

# Adding the new column at the start of the dataframe
panel_a <- panel_a %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
kable(panel_a, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel A: 1947:Q1–2023:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_a, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel B -----------------------------------------------

df_inflation_complete_B <- df_inflation_complete %>%
  filter(FirstDate >= "1986-01-01" & FirstDate <= "2023-10-01")

punew_year <- df_inflation_complete_B %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_complete_B %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_complete_B %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_complete_B %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

df_inflation_complete_yearly_B <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(df_inflation_complete_yearly_B, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_complete_B %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_b <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_b <- panel_b %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel B (latex code)
kable(panel_b, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel B: 1986:Q1– 2023:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_b, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel C -----------------------------------------------

df_inflation_complete_C <- df_inflation_complete %>%
  filter(FirstDate >= "1996-01-01" & FirstDate <= "2023-10-01")

punew_year <- df_inflation_complete_C %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_complete_C %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_complete_C %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_complete_C %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

df_inflation_complete_yearly_C <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(df_inflation_complete_yearly_C, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_complete_C %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_c <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_c <- panel_c %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel C (latex code)
kable(panel_c, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel C: 1996:Q1– 2023:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)


# Clear environment
rm(panel_c, sds, means, autocorrelation_quaterly, corr_table, data_variables)
rm(df_inflation_complete_B, df_inflation_complete_C, df_inflation_complete_yearly_B, df_inflation_complete_yearly_C, statistics_labels)

## 04.1 Recreate figure 1.A -----------------------------

# Multiply the inflation columns by 100

data_plot1A <- df_inflation_authors_yearly %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  mutate(across(ends_with("year"), ~ .x * 100)) %>%
  full_join(., df_livingston_complete)  %>%
  filter(group < 2003 & group >1951)


# Pivot the data to a long format for plotting with ggplot2
data_plot1A_long <- data_plot1A %>%
  pivot_longer(cols = ends_with("year"), names_to = "inflation_type", values_to = "inflation_value") %>%
  select(FirstDate, inflation_type, inflation_value) %>%
  arrange(FirstDate)

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "longdash", "dotted", "dotdash", NA)
shapes <- c(NA, NA, NA, NA, 3) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to line1 types
names(line_types) <- unique(data_plot1A_long$inflation_type)
names(shapes) <- unique(data_plot1A_long$inflation_type)

# Plot the data
plot_1a <- ggplot(data_plot1A_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_1a

rm(plot_1a, data_plot1A, data_plot1A_long, line_types, shapes)

## 04.2 Recreate figure 1.B -----------------------------

data_plot1B <- df_inflation_authors_yearly %>%
  mutate(across(ends_with("punew_year"), ~ .x * 100)) %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  filter(group >= 1978 & group <= 2002) %>%
  left_join(df_surveys_complete) %>%
  select(FirstDate, quarter, group, punew_year, liv_year, mich_year, spf_year)

# Pivot the data to a long format for plotting with ggplot2
data_plot1B_long <- data_plot1B %>%
  pivot_longer(cols = ends_with("year"), names_to = "inflation_type", values_to = "inflation_value") %>%
  select(FirstDate, inflation_type, inflation_value)

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "dotted", "dotdash", "longdash")
shapes <- c(NA, NA, NA, NA) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to linetypes
names(line_types) <- unique(data_plot1B_long$inflation_type)
names(shapes) <- unique(data_plot1B_long$inflation_type)

# Plot the data
plot_1B <- ggplot(data_plot1B_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_1B

rm(plot_1B, data_plot1B, data_plot1B_long, line_types, shapes)

## 04.3 Recreate figure 1.A (extended) -----------------------------

# Multiply the inflation columns by 100

data_plot1A <- df_inflation_authors_yearly %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  mutate(across(ends_with("year"), ~ .x * 100)) %>%
  full_join(., df_livingston_complete)

# Pivot the data to a long format for plotting with ggplot2
data_plot1A_long <- data_plot1A %>%
  pivot_longer(cols = ends_with("year"), names_to = "inflation_type", values_to = "inflation_value") %>%
  select(FirstDate, inflation_type, inflation_value) %>%
  arrange(FirstDate)

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "longdash", "dotted", "dotdash", NA)
shapes <- c(NA, NA, NA, NA, 3) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to line1 types
names(line_types) <- unique(data_plot1A_long$inflation_type)
names(shapes) <- unique(data_plot1A_long$inflation_type)

# Plot the data
plot_1a <- ggplot(data_plot1A_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_1a

rm(plot_1a, data_plot1A, data_plot1A_long, line_types, shapes)


## 04.2 Recreate figure 1.B (extended)-----------------------------

data_plot1B <- df_inflation_authors_yearly %>%
  mutate(across(ends_with("punew_year"), ~ .x * 100)) %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  left_join(df_surveys_complete) %>%
  select(FirstDate, quarter, group, punew_year, liv_year, mich_year, spf_year)

# Pivot the data to a long format for plotting with ggplot2
data_plot1B_long <- data_plot1B %>%
  pivot_longer(cols = ends_with("year"), names_to = "inflation_type", values_to = "inflation_value") %>%
  select(FirstDate, inflation_type, inflation_value)

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "dotted", "dotdash", "longdash")
shapes <- c(NA, NA, NA, NA) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to linetypes
names(line_types) <- unique(data_plot1B_long$inflation_type)
names(shapes) <- unique(data_plot1B_long$inflation_type)

# Plot the data
plot_1B <- ggplot(data_plot1B_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_1B

rm(plot_1B, data_plot1B, data_plot1B_long, line_types, shapes)
```

```{r replication_a1_summary-stats_figures}

# missing packages

## 02.1 Recreate table 1 Summary Statistics - original --------------------------

### Panel A --------------------------------------------------

data_variables <- select(df_inflation_authors_yearly, ends_with('year'))

# Calculate summary statistics
means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_a <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# New column to add at the start
statistics_labels <- c("Mean", "Standard deviation", "Autocorrelation", "Correlations", "PUXHS", "PUXX", "PCE")

# Adding the new column at the start of the dataframe
panel_a <- panel_a %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
kable(panel_a, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel A: 1952:Q2–2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_a, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel B -----------------------------------------------

df_inflation_authors_B <- df_inflation_authors %>%
  filter(FirstDate >= "1986-01-01" & FirstDate <= "2002-10-01")

punew_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors_B %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

df_inflation_authors_yearly_B <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(df_inflation_authors_yearly_B, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors_B %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_b <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_b <- panel_b %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel B (latex code)
kable(panel_b, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel B: 1986:Q1– 2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_b, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel C -----------------------------------------------

df_inflation_authors_C <- df_inflation_authors %>%
  filter(FirstDate >= "1996-01-01" & FirstDate <= "2002-10-01")

punew_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_authors_C %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

df_inflation_authors_yearly_C <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(df_inflation_authors_yearly_C, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_authors_C %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_c <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_c <- panel_c %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel C (latex code)
kable(panel_c, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel C: 1996:Q1– 2002:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)


# Clear environment
rm(panel_c, sds, means, autocorrelation_quaterly, corr_table, data_variables)
rm(df_inflation_authors_B, df_inflation_authors_C, df_inflation_authors_yearly_B, df_inflation_authors_yearly_C, statistics_labels)

## 02.2 Recreate table 1 Summary Statistics - complete --------------------------

### Panel A --------------------------------------------------

data_variables <- select(df_inflation_complete_yearly, ends_with('year'))

# Calculate summary statistics
means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_complete %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_a <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# New column to add at the start
statistics_labels <- c("Mean", "Standard deviation", "Autocorrelation", "Correlations", "PUXHS", "PUXX", "PCE")

# Adding the new column at the start of the dataframe
panel_a <- panel_a %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel A (latex code)
kable(panel_a, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel A: 1947:Q1–2023:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_a, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel B -----------------------------------------------

df_inflation_complete_B <- df_inflation_complete %>%
  filter(FirstDate >= "1986-01-01" & FirstDate <= "2023-10-01")

punew_year <- df_inflation_complete_B %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_complete_B %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_complete_B %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_complete_B %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

df_inflation_complete_yearly_B <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(df_inflation_complete_yearly_B, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_complete_B %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_b <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_b <- panel_b %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel B (latex code)
kable(panel_b, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel B: 1986:Q1– 2023:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)

rm(panel_b, sds, means, autocorrelation_quaterly, corr_table, data_variables)

### Panel C -----------------------------------------------

df_inflation_complete_C <- df_inflation_complete %>%
  filter(FirstDate >= "1996-01-01" & FirstDate <= "2023-10-01")

punew_year <- df_inflation_complete_C %>%
  group_by(group) %>%
  summarise(punew_year = sum(inflation_punew, na.rm = TRUE))
puxhs_year <- df_inflation_complete_C %>%
  group_by(group) %>%
  summarise(puxhs_year = sum(inflation_puxhs, na.rm = TRUE))
puxx_year <- df_inflation_complete_C %>%
  group_by(group) %>%
  summarise(puxx_year = sum(inflation_puxx, na.rm = TRUE))
pce_year <- df_inflation_complete_C %>%
  group_by(group) %>%
  summarise(pce_year = sum(inflation_pce, na.rm = TRUE))

df_inflation_complete_yearly_C <- full_join(punew_year, puxhs_year, by = "group") %>%
  full_join(., puxx_year, by = "group") %>%
  full_join(., pce_year, by = "group") %>%
  mutate(across(everything(), ~na_if(.x, 0)))

rm(pce_year, punew_year, puxhs_year, puxx_year)

data_variables <- select(df_inflation_complete_yearly_C, ends_with('year'))

means <- 100*round(colMeans(data_variables, na.rm = T),4)
sds <- data_variables %>% summarise(across(everything(), ~ 100* sd(., na.rm=T)))
autocorrelation_quaterly <- df_inflation_complete_C %>% 
  select(starts_with("inflation")) %>%
  summarise(across(everything(), ~ cor(., lag(., 4), use='complete.obs'))) %>%
  select("punew_year" = "inflation_punew", "puxhs_year" = "inflation_puxhs", "puxx_year" = "inflation_puxx", "pce_year" = "inflation_pce")
corr_table <- round(cor(data_variables, use = 'complete.obs'), 2)
corr_table[!lower.tri(corr_table)] <- NA

panel_c <- bind_rows(list(means, sds, autocorrelation_quaterly, as.data.frame(corr_table)))

# Adding the new column at the start of the dataframe
panel_c <- panel_c %>%
  mutate(Statistic = statistics_labels) %>%
  select(Statistic, everything()) %>%
  mutate(across(where(is.numeric), ~ round(., 2)))

# Generate the table 1, panel C (latex code)
kable(panel_c, "html", booktabs = TRUE, align = 'c', col.names = c("", "PUNEW", "PUXHS", "PUXX", "PCE")) %>%
  kable_styling(latex_options = c("striped", "hold_position")) %>%
  add_header_above(c(" " = 1, "Panel C: 1996:Q1– 2023:Q4" = 4)) %>%
  pack_rows("Mean", 1, 1) %>%
  pack_rows("Standard deviation", 2, 2) %>%
  pack_rows("Autocorrelation", 3, 3) %>%
  pack_rows("Correlations", 4, 6)


# Clear environment
rm(panel_c, sds, means, autocorrelation_quaterly, corr_table, data_variables)
rm(df_inflation_complete_B, df_inflation_complete_C, df_inflation_complete_yearly_B, df_inflation_complete_yearly_C, statistics_labels)

## 04.1 Recreate figure 1.A -----------------------------

# Multiply the inflation columns by 100

data_plot1A <- df_inflation_authors_yearly %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  mutate(across(ends_with("year"), ~ .x * 100)) %>%
  full_join(., df_livingston_complete)  %>%
  filter(group < 2003 & group >1951)


# Pivot the data to a long format for plotting with ggplot2
data_plot1A_long <- data_plot1A %>%
  pivot_longer(cols = ends_with("year"), names_to = "inflation_type", values_to = "inflation_value") %>%
  select(FirstDate, inflation_type, inflation_value) %>%
  arrange(FirstDate)

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "longdash", "dotted", "dotdash", NA)
shapes <- c(NA, NA, NA, NA, 3) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to line1 types
names(line_types) <- unique(data_plot1A_long$inflation_type)
names(shapes) <- unique(data_plot1A_long$inflation_type)

# Plot the data
plot_1a <- ggplot(data_plot1A_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_1a

rm(plot_1a, data_plot1A, data_plot1A_long, line_types, shapes)

## 04.2 Recreate figure 1.B -----------------------------

data_plot1B <- df_inflation_authors_yearly %>%
  mutate(across(ends_with("punew_year"), ~ .x * 100)) %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  filter(group >= 1978 & group <= 2002) %>%
  left_join(df_surveys_complete) %>%
  select(FirstDate, quarter, group, punew_year, liv_year, mich_year, spf_year)

# Pivot the data to a long format for plotting with ggplot2
data_plot1B_long <- data_plot1B %>%
  pivot_longer(cols = ends_with("year"), names_to = "inflation_type", values_to = "inflation_value") %>%
  select(FirstDate, inflation_type, inflation_value)

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "dotted", "dotdash", "longdash")
shapes <- c(NA, NA, NA, NA) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to linetypes
names(line_types) <- unique(data_plot1B_long$inflation_type)
names(shapes) <- unique(data_plot1B_long$inflation_type)

# Plot the data
plot_1B <- ggplot(data_plot1B_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_1B

rm(plot_1B, data_plot1B, data_plot1B_long, line_types, shapes)

## 04.3 Recreate figure 1.A (extended) -----------------------------

# Multiply the inflation columns by 100

data_plot1A <- df_inflation_authors_yearly %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  mutate(across(ends_with("year"), ~ .x * 100)) %>%
  full_join(., df_livingston_complete)

# Pivot the data to a long format for plotting with ggplot2
data_plot1A_long <- data_plot1A %>%
  pivot_longer(cols = ends_with("year"), names_to = "inflation_type", values_to = "inflation_value") %>%
  select(FirstDate, inflation_type, inflation_value) %>%
  arrange(FirstDate)

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "longdash", "dotted", "dotdash", NA)
shapes <- c(NA, NA, NA, NA, 3) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to line1 types
names(line_types) <- unique(data_plot1A_long$inflation_type)
names(shapes) <- unique(data_plot1A_long$inflation_type)

# Plot the data
plot_1a <- ggplot(data_plot1A_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_1a

rm(plot_1a, data_plot1A, data_plot1A_long, line_types, shapes)


## 04.2 Recreate figure 1.B (extended)-----------------------------

data_plot1B <- df_inflation_authors_yearly %>%
  mutate(across(ends_with("punew_year"), ~ .x * 100)) %>%
  mutate(quarter = as.yearqtr(FirstDate)) %>%
  left_join(df_surveys_complete) %>%
  select(FirstDate, quarter, group, punew_year, liv_year, mich_year, spf_year)

# Pivot the data to a long format for plotting with ggplot2
data_plot1B_long <- data_plot1B %>%
  pivot_longer(cols = ends_with("year"), names_to = "inflation_type", values_to = "inflation_value") %>%
  select(FirstDate, inflation_type, inflation_value)

# Define linetypes and shapes based on the provided plot image
line_types <- c("solid", "dotted", "dotdash", "longdash")
shapes <- c(NA, NA, NA, NA) # Only the 'Livingston' series uses a shape, represented by pluses

# Create a named vector to map the inflation types to linetypes
names(line_types) <- unique(data_plot1B_long$inflation_type)
names(shapes) <- unique(data_plot1B_long$inflation_type)

# Plot the data
plot_1B <- ggplot(data_plot1B_long, aes(x = FirstDate, y = inflation_value, color = inflation_type, linetype = inflation_type, shape = inflation_type)) +
  geom_line() +
  geom_point(size = 3) +
  scale_color_manual(values = c("black", "black", "black", "black")) +
  scale_linetype_manual(values = line_types) +
  scale_shape_manual(values = shapes) +
  labs(x = "Year", y = "Percentage", title = "Inflation Over Time") +
  theme_minimal() +
  theme(legend.title = element_blank(),
        legend.position = "top",
        axis.text.x = element_text(angle = 45, hjust = 1))

plot_1B

rm(plot_1B, data_plot1B, data_plot1B_long, line_types, shapes)

```

The results of these scripts can been seen by checking the folders 'table' and 'figures' on the github repository. We were unalbe to print their results directly into the markdown file.

## Estimating the models

### Survey analysis

On the following script, there are the necessary values to construct table 3, and table 4. For better understanding, We recommend to run it separately.

```{r}
source("replication_a1_survey_models.R")
```


### Time-Series Models

Here we'll estimate the ARMA, AR, RW and AORW.

```{r time_series_models}

# 02 load dataframes used ------------------------

df_inflation_complete <- read.csv(file = "data/output/df_inflation_complete.csv")
df_inflation_authors <- read.csv(file = "data/output/df_inflation_authors.csv")
df_inflation_authors_yearly <- read.csv(file = "data/output/df_inflation_authors_yearly.csv")

# 03 models ------------------------------------

# Function to perform rolling window forecasting with ARMA(1,1) over years
rolling_forecast_arma <- function(data, series_name, order_ar=1, order_ma=1, initial_end=1985, final_date=2002, window_size=4, return_rmse=T) {
  # adding a forecast vector to store results
  forecasts <- numeric((final_date - initial_end)*window_size)
  
  # loop for the rolling window
  for (y in initial_end:(final_date-1)) {  # we take one out to don't count the last year
    train_data <- data %>% filter(group <= y)  # Use data up to the current index for training
    train_series <- train_data %>% .[series_name]
    
    # Fit ARMA(1,1) model: yt = mu + phi*yt-1 + epsilon_t + psi*epsilon_{t-1}
    arma_model <- arima(train_series, order = c(order_ar, 0, order_ma))
    # get coefficients + residuals
    phi <- arma_model$coef[1]
    psi <- arma_model$coef[2]
    mu <- arma_model$coef[3]
    residuals <- arma_model$residuals
    
    # Forecast for the next 4 quarters according to equation on paper
    forecast_q <- c()
    for (q in 1:4) {
      forecast_q[q] <- (1/(1-phi)) * (4 - (phi * (1-phi^4))/(1-phi)) * mu + ((phi * (1-phi^4))/(1-phi)) * train_series[nrow(train_series) - (q-1),] + (psi * (1-phi^4))/(1-phi) * residuals[nrow(train_series) - (q-1)]
    }
    
    # just making sure
    forecast_values <- as.numeric(forecast_q)
    # Store forecast values
    forecasts[((y - initial_end)*window_size + 1):((y - initial_end+1)*window_size)] <- forecast_values
  }
  
  # decide whether to return forecasts or root mean squared errors (RMSE)
  if (return_rmse) {
    # get data to validate forecasts
    test_series <- data %>% filter(group > initial_end) %>% .[series_name]
    rmse <- sqrt(sum((test_series - forecasts)^2, na.rm=T))
    return(rmse)
  } else{
    return(forecasts)
  }
}

# very similar to the last function, 
# with the difference that now we'll choose AR(p) recursively by SIC
rolling_forecast_ar_p <- function(data, series_name, initial_end=1985, final_date=2002, window_size=4, return_rmse=T) {
  # creating a forecast vector to store results
  forecasts <- numeric((final_date - initial_end)*window_size)
  
  # loop for the rolling window
  for (y in initial_end:(final_date-1)) {
    train_data <- data %>% filter(group <= y)  # Use data up to the current index for training
    train_series <- train_data %>% select(series_name)
    
    # find best ar order
    ar_order <- best_arma_order(train_series)
    # Fit AR(p) model
    arma_model <- stats::arima(train_series, order = c(ar_order, 0, 0))
    
    # Forecast for the next 4 quarters
    #  ... a little note: here we abstract from the way the paper is doing, since it would imply unecessary complexity (look at the equation for the quarterly forecast in page 1174)
    forecast_values <- as.numeric(forecast(arma_model, h = 4)$mean)
    # Store forecast values
    forecasts[((y - initial_end)*window_size + 1):((y - initial_end+1)*window_size)] <- forecast_values
  }
  
  # decide whether to return forecasts or root mean squared errors (RMSE)
  if (return_rmse) {
    # get data to validate forecasts
    test_series <- data %>% filter(group > initial_end) %>% .[series_name]
    rmse <- sqrt(sum((test_series - forecasts)^2, na.rm=T))
    return(rmse)
  } else{
    return(forecasts)
  }
}

# to find the best arma model based on some criteria (still don't contemplate ma orders or other info_criteria than BIC/SIC)
best_arma_order <- function(series, ar_until = 10, ma_order = 0, info_criteria = 'bic'){
  arma_models <- list()
  bic_arma <- c()
  for (p in 1:ar_until) {
    arma_models[[p]] <- arima(series, order = c(p, 0, ma_order))
    # BIC = SIC
    bic_arma[p] <- BIC(arma_models[[p]])
  }
  arma_best_order <- which(min(bic_arma) == bic_arma)
  return(arma_best_order)
}

# creates a matrix with (y*x) rows and (estats) columns
create_table_inflation_models <- function(values_estats, y, x, estats=c('relative_rmse', '1-lambda', 'hh_error', 'nw_error')){
  # values_stats should be a list of same size as the estats, each value being y*x size (in that order)
  # storing data for table
  table_paper <- matrix(NA, length(y)*length(x), length(estats))  # just for a single estimation period
  # prettify
  colnames(table_paper) <- estats
  names_inflation <- rep(y, each=length(x))
  names_models <- rep(x, length(y))
  rownames(table_paper) <- paste(names_inflation, names_models, sep=' | ')
  
  # adding values
  for (p in 1:length(estats)) {
    table_paper[,p] <- values_estats[[p]]
  }
  
  return(table_paper)
}

# running the functions
inflation_series <- df_inflation_authors %>% select(starts_with('inflation')) %>% colnames()

# loops

# defining vectors to store data
arma11_rmse1985 <- c()
arp_rmse <- c()
arma11_forecasts1985 <- list()

# 1985
for (x in 1:length(inflation_series)) {
  print(inflation_series[x])
  # first the simple rmse values for each inflation series
  arma11_rmse1985[x] <- rolling_forecast_arma(df_inflation_authors, inflation_series[x])
  arp_rmse[x] <- rolling_forecast_ar_p(df_inflation_authors, inflation_series[x])
  
  # we need also the forecasts of the arma11 model to run the lambda models
  arma11_forecasts1985[[inflation_series[x]]] <- rolling_forecast_arma(df_inflation_authors, inflation_series[x], return_rmse = F)
}

# printing info
arma11_rmse <- arma11_rmse1985
#
# a loop to get values in the way the table is constructed in the paper
just_rmse <- c()
relative_rmse <- c()
for (x in 1:length(arma11_rmse)) {
  just_rmse <- c(just_rmse, arma11_rmse[x], arp_rmse[x])
  relative_rmse_aux <- c(arma11_rmse[x], arp_rmse[x])
  relative_rmse <- c(relative_rmse, relative_rmse_aux)
}
values_estats <- list(just_rmse, relative_rmse)
table_ts <- create_table_inflation_models(values_estats, inflation_series, c('ARMA', 'AR'), c('RMSE', 'ARMA=1'))
print(table_ts)

# ---
# again for a new year
# ---

# todo: add option to pick the values 

arma11_rmse1995 <- c()
arp_rmse <- c()
arma11_forecasts1995 <- list()

# 1995
for (x in 1:length(inflation_series)) {
  print(inflation_series[x])
  # first the simple rmse values for each inflation series
  arma11_rmse1995[x] <- rolling_forecast_arma(df_inflation_authors, inflation_series[x], initial_end = 1995)
  arp_rmse[x] <- rolling_forecast_ar_p(df_inflation_authors, inflation_series[x])
  
  # we need also the forecasts of the arma11 model to run the lambda models
  arma11_forecasts1995[[inflation_series[x]]] <- rolling_forecast_arma(df_inflation_authors, inflation_series[x], return_rmse = F, initial_end = 1995)
}

# printing info
arma11_rmse <- arma11_rmse1995
#
# a loop to get values in the way the table is constructed in the paper
just_rmse <- c()
relative_rmse <- c()
for (x in 1:length(arma11_rmse)) {
  just_rmse <- c(just_rmse, arma11_rmse[x], arp_rmse[x])
  relative_rmse_aux <- c(arma11_rmse[x], arp_rmse[x])
  relative_rmse <- c(relative_rmse, relative_rmse_aux)
}
values_estats <- list(just_rmse, relative_rmse)
table_ts_1995 <- create_table_inflation_models(values_estats, inflation_series, c('ARMA', 'AR'), c('RMSE', 'ARMA=1'))
print(table_ts_1995)
```

### Phillips Curve (OLS) + Time Structure models


Here, we do a similar procedure as before for the time-series models, however, now we estimate the models using a simple OLS regression. As for the AR(p) model we'll dynamically choose the order of the regressors such that it minimizes the bayesian information criteria (BIC or SIC). Basically, the model we'll estimate goes as follows (for the training sample):

$$
\pi_{t+4,4} = \alpha + \beta'(L) X_t + \epsilon_{t+4,4}
$$
where $\pi_{t+4,4}$ is defined (annually) as previously, $\beta(L)$ is a filter for the variables in $X_t$ that includes the dependent variable (inflation) and another independent variable (some real activity measure, $y_{t+4,4}$). That means that if we have $L=2$, $\beta'(L) X_t$ will include $\pi_{t+3,4}$, $\pi_{t+2,4}$, $y_{t+3,4}$ and $y_{t+2,4}$.

Regarding the window of training and testing, we weren't able to identify whether the authors keep, as before, a rolling window or whether they now use a fixed one. Despite it would make sense to keep the previous approach, we choose to use a fixed window to keep with a conservative/simplicity take.

Moreover, since the process is basically the same as before for the term structure models, we just add them below, then printing the tables.

As previously, we first do for the 1985 period, then for 1995.

```{r phillips_term-structure1985}
# ---
# Load data ----
# ---

# df_inflation_complete <- read.csv("df_inflation_complete.csv")
df_inflation_authors <- read.csv("data/output/df_inflation_authors.csv")
df_realmeasures_authors <- read.csv("data/output/df_realmeasures_authors.csv")

# Prepare data
df_phillips <- df_inflation_authors %>%
  left_join(df_realmeasures_authors) %>% 
  tibble() %>%
  arrange(FirstDate)

# ---
# Functions ----
# ---

# a function just to lag our dataset
lag_df <- function(df, y, x, lag_until = 5) {
  models <- list()
  bic_models <- c()
  df_lag <- df
  
  # first we loop for the y variable
  for (p in 1:lag_until) {
    df_lag <- df_lag %>% 
      mutate(!!str_c('lag', p, '_', y) := lag(!!sym(y), p))
  }
  
  # then we check whether x is a vector and do the adequate operation 
  if (is.character(x) && length(x) == 1) {
    # If x is a single string, create lagged column for it
    for (p in 1:lag_until) {
      df_lag <- df_lag %>% 
        mutate(!!str_c('lag', p, '_', x) := lag(!!sym(x), p))
    }
  } else if (is.character(x) && length(x) > 1) {
    # If x is a character vector, create lagged columns for each element
    for (var in x) {
      for (p in 1:lag_until) {
        df_lag <- df_lag %>% 
          mutate(!!str_c('lag', p, '_', var) := lag(!!sym(var), p))
      }
    }
  }
  
  return(df_lag)
}

# a function to choose the best lag of the ols based on bic
best_lag_ols <- function(df_lag, y, lag_until=5, criteria='bic'){
  # defining relevant variables
  dependent_var <- y
  independent_var <- c()
  
  # variables to store
  models <- list()
  bic_models <- c()
  
  # loop of lags
  for (p in 1:lag_until) {
    independent_var_plus <- df_lag %>% select(starts_with(paste0('lag', p))) %>% colnames()
    independent_var <- c(independent_var, independent_var_plus)
    reg_formula <- formula(paste(dependent_var, '~', paste(independent_var, collapse=' + ')))
    models[[p]] <- lm(reg_formula, data=df_lag)
    bic_models[p] <- BIC(models[[p]])
  }
  
  # then find the best order
  best_order <- which(min(bic_models) == bic_models)
  
  return(models[[best_order[1]]])
}

# todo: use a rolling window?
# a function to predict for a ols model
ols_predict <- function(train_data, test_data, y, x, max_lag=5){
  # choose the best lag and returns the model
  model <- best_lag_ols(train_data, y, lag_until = max_lag)
  
  # forecast values
  forecast_values <- forecast(model, test_data)
  
  return(forecast_values$mean)
}

# creates a matrix with (y*x) rows and (estats) columns
create_table_inflation_models <- function(values_estats, y, x, estats=c('relative_rmse', '1-lambda', 'hh_error', 'nw_error')){
  # values_stats should be a list of same size as the estats, each value being y*x size (in that order)
  # storing data for table
  table_paper <- matrix(NA, length(y)*length(x), length(estats))  # just for a single estimation period
  # prettify
  colnames(table_paper) <- estats
  names_inflation <- rep(y, each=length(x))
  names_models <- rep(x, length(y))
  rownames(table_paper) <- paste(names_inflation, names_models, sep=' | ')
  
  # adding values
  for (p in 1:length(estats)) {
    table_paper[,p] <- values_estats[[p]]
  }
  
  return(table_paper)
}

# ---
# Run ----
# ---

# change here to change periods (either 1985 or 1995 yet)
initial_estimation_period <- 1985
arma11_rmse <- arma11_rmse1985
arma11_forecasts <- arma11_forecasts1985

# ---
## Phillips models ----
# ---

all_y <- df_phillips %>% select(starts_with('inflation')) %>% colnames()  # "inflation_punew" "inflation_puxhs" "inflation_puxx"  "inflation_pce"
x_phillips <- df_phillips %>% select(gdpg:fac) %>% colnames()  # "gdpg"            "gap1"            "gap2"            "unrate"          "lshr"            "xli"             "xli2"            "fac"

# todo: we need more one loop for the other group: 1995

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
for (y in all_y) {
  print(y)
  for (x in x_phillips) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    arma11_forecasts_now <- arma11_forecasts[[y]]
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts_now + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_phillips))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_ols <- create_table_inflation_models(values_estats, all_y, x_phillips)
print(table_ols)

# ---
# now do it for models with double x's
# ---
doublex_phillips <- list(c('gap1', 'lshr'), c('gap2', 'lshr'))
x_names <- sapply(doublex_phillips, function(x) paste(x, collapse=' + '))
x_now <- doublex_phillips
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
# loop
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    arma11_forecasts_now <- arma11_forecasts[[y]]     
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts_now + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_ols_2x <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_ols_2x)

# ---
## Term Structure ----
# ---

# basically the same as before, plus rate

all_y <- df_phillips %>% select(starts_with('inflation')) %>% colnames()  # "inflation_punew" "inflation_puxhs" "inflation_puxx"  "inflation_pce"
x_phillips <- df_phillips %>% select(gdpg:fac) %>% colnames()  # "gdpg"            "gap1"            "gap2"            "unrate"          "lshr"            "xli"             "xli2"            "fac"
x_with_rate <- lapply(x_phillips, function(x) c(x, "rate"))
x_names <- sapply(x_with_rate, function(x) paste(x, collapse=' + '))
x_now <- x_with_rate

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    arma11_forecasts_now <- arma11_forecasts[[y]]     
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts_now + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_term_structure <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_term_structure)

# ---
# now do it for some other specific
# ---
term_structure_x_specific <- list(c('yield'), c('rate', 'yield'), c('gdpg', 'rate', 'yield'))
x_names <- sapply(term_structure_x_specific, function(x) paste(x, collapse=' + '))
x_now <- term_structure_x_specific
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
# loop
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    arma11_forecasts_now <- arma11_forecasts[[y]]     
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts_now + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_term_structure_specific <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_term_structure_specific)

```

Now for 1995.

```{r phillips_term-structure1995}

# ---
# Run ----
# ---

# change here to change periods (either 1985 or 1995 yet)
initial_estimation_period <- 1995
arma11_rmse <- arma11_rmse1995
arma11_forecasts <- arma11_forecasts1995

# ---
## Phillips models ----
# ---

all_y <- df_phillips %>% select(starts_with('inflation')) %>% colnames()  # "inflation_punew" "inflation_puxhs" "inflation_puxx"  "inflation_pce"
x_phillips <- df_phillips %>% select(gdpg:fac) %>% colnames()  # "gdpg"            "gap1"            "gap2"            "unrate"          "lshr"            "xli"             "xli2"            "fac"

# todo: we need more one loop for the other group: 1995

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
for (y in all_y) {
  print(y)
  for (x in x_phillips) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    arma11_forecasts_now <- arma11_forecasts[[y]]
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts_now + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_phillips))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_ols <- create_table_inflation_models(values_estats, all_y, x_phillips)
print(table_ols)

# ---
# now do it for models with double x's
# ---
doublex_phillips <- list(c('gap1', 'lshr'), c('gap2', 'lshr'))
x_names <- sapply(doublex_phillips, function(x) paste(x, collapse=' + '))
x_now <- doublex_phillips
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
# loop
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    arma11_forecasts_now <- arma11_forecasts[[y]]     
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts_now + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_ols_2x <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_ols_2x)

# ---
## Term Structure ----
# ---

# basically the same as before, plus rate

all_y <- df_phillips %>% select(starts_with('inflation')) %>% colnames()  # "inflation_punew" "inflation_puxhs" "inflation_puxx"  "inflation_pce"
x_phillips <- df_phillips %>% select(gdpg:fac) %>% colnames()  # "gdpg"            "gap1"            "gap2"            "unrate"          "lshr"            "xli"             "xli2"            "fac"
x_with_rate <- lapply(x_phillips, function(x) c(x, "rate"))
x_names <- sapply(x_with_rate, function(x) paste(x, collapse=' + '))
x_now <- x_with_rate

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    arma11_forecasts_now <- arma11_forecasts[[y]]     
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts_now + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_term_structure <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_term_structure)

# ---
# now do it for some other specific
# ---
term_structure_x_specific <- list(c('yield'), c('rate', 'yield'), c('gdpg', 'rate', 'yield'))
x_names <- sapply(term_structure_x_specific, function(x) paste(x, collapse=' + '))
x_now <- term_structure_x_specific
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
# loop
for (y in all_y) {
  print(y)
  for (x in x_now) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    arma11_forecasts_now <- arma11_forecasts[[y]]     
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts_now + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_names))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_term_structure_specific <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_term_structure_specific)

```


## Brazilian data

Describe data processing


Repeat all models for Brazilian data.

### Time-Series Models

We pick as 2015 as the point of division of our inflation train to test sample. We let our data run up until the first quarter of 2023 (indeed, as the authors, we let our data to be at that frequency, although we have at a monthly frequency). This makes a division of approximately 2/3 (train/test), because we let the data to start to run at 1999 (to avoid periods of hyperinflation and to make all series equally available). We pick three series: IPCA, IPCA-15 and EXFE. However, changing some of those specifications don't lead us to different conclusions.

The models indicate that AR(p) models always perform better than the ARMA(1,1) models by considerable margin.

```{r ts_brazil}

# load data
df_inflation_brazil <- read_csv('brazil_data/df_inflation_brazil.csv') %>% 
  filter(group >= 1999)

# new series!
inflation_series <- c('ipca', 'ipca_15', 'exfe')

# values to store
arma11_rmse_brazil <- c()
arp_rmse_brazil <- c()
arma11_forecasts_brazil <- list()
new_start <- 2015
new_end <- 2023
# same models as previously
for (x in 1:length(inflation_series)) {
  print(inflation_series[x])
  # first the simple rmse values for each inflation series
  arma11_rmse_brazil[x] <- rolling_forecast_arma(df_inflation_brazil, inflation_series[x], initial_end = new_start, final_date = new_end)
  arp_rmse_brazil[x] <- rolling_forecast_ar_p(df_inflation_brazil, inflation_series[x], initial_end = new_start, final_date = new_end)
  
  # we need also the forecasts of the arma11 model to run the lambda models
  arma11_forecasts_brazil[[inflation_series[x]]] <- rolling_forecast_arma(df_inflation_brazil, inflation_series[x], return_rmse = F, initial_end = new_start, final_date = new_end)
}

# printing info
arma11_rmse <- arma11_rmse_brazil
#
# a loop to get values in the way the table is constructed in the paper
just_rmse <- c()
relative_rmse <- c()
for (x in 1:length(arma11_rmse)) {
  just_rmse <- c(just_rmse, arma11_rmse[x], arp_rmse_brazil[x])
  relative_rmse_aux <- c(arma11_rmse[x]/arma11_rmse[x], arp_rmse_brazil[x]/arma11_rmse[x])
  relative_rmse <- c(relative_rmse, relative_rmse_aux)
}
values_estats <- list(just_rmse, relative_rmse)
table_ts_brazil <- create_table_inflation_models(values_estats, inflation_series, c('ARMA', 'AR'), c('RMSE', 'ARMA=1'))
print(table_ts_brazil)

```


### OLS: Phillips Curve + Term Structure Models

Same periods of analysis as in the previous time-series models. There doesn't seem to be a dominant series, but in general they perform better than the ARMA(1,1). Nevertheless, as we saw, AR(p) models are better than the ARMA(1,1) models (different from what we see in the paper).

```{r phillips_term-structure_brazil}
# load data
df_inflation_brazil <- read.csv("brazil_data/df_inflation_brazil.csv")
df_realmeasures_brazil <- read.csv("brazil_real_measures_data/df_realmeasures_brazil.csv")

# Prepare data
df_phillips_brazil <- df_inflation_brazil %>%
  left_join(df_realmeasures_brazil) %>% 
  tibble() %>%
  arrange(FirstDate) %>% 
  filter(group <= 2023, group >=1999)

# new definitions
all_y <- c('ipca', 'ipca_15', 'exfe')
x_brazil <- list("gdpg", "gap1", "gap2", "unrate", "lshr", c('gap1', 'lshr'), c('gap2', 'lshr'))
x_names <- sapply(x_brazil, function(x) paste(x, collapse=' + '))
initial_estimation_period <- 2015
arma11_rmse <- arma11_rmse_brazil
arma11_forecasts <- arma11_forecasts_brazil

# the loops for the phillips curve models

# two loops, one for the dependent variable, other for the independent variable
rmse <- c()
lambda_phillips <- c()
hh_se_phillips <- c()
nw_se_phillips <- c()
for (y in all_y) {
  print(y)
  for (x in x_brazil) {
    print(x)
    # define all the dataframes relevant
    df_lag <- lag_df(df_phillips_brazil, y, x)
    train_data <- df_lag %>% filter(group <= initial_estimation_period)
    test_data <- df_lag %>% filter(group > initial_estimation_period)
    
    # get forecast
    forecast_values <- ols_predict(train_data, test_data, y, x)
    
    # todo: store the arma11 rmse from previous exercise
    # compute (relative) rmse
    rmse <- c(rmse, sqrt(sum((test_data[y] - forecast_values)^2, na.rm=T)))
    
    # compute 1-lambda
    arma11_forecasts_now <- arma11_forecasts[[y]]
    reg_formula_lambda <- formula(paste0(y, '~ arma11_forecasts_now + forecast_values'))
    model_lambda <- lm(reg_formula_lambda, data=test_data)
    lambda_phillips <- c(lambda_phillips ,coef(model_lambda)[3])
    # standard-error first with HH-error
    hh_se_phillips <- c(hh_se_phillips ,kernHAC(model_lambda, kernel = "Truncated")[3,3])
    # then with NW-error
    nw_se_phillips <- c(nw_se_phillips, NeweyWest(model_lambda)[3,3])
  }
}

# calculate the relative rmse to arma11
relative_rmse <- rmse/rep(arma11_rmse, each=length(x_brazil))
# define a list to pass to the function to create tables paper-like
values_estats <- list(relative_rmse, lambda_phillips, hh_se_phillips, nw_se_phillips)
table_ols <- create_table_inflation_models(values_estats, all_y, x_names)
print(table_ols)
```

## Critical part

First of all, we very thankful to the authors kindness in sharing with us the data to replicate the paper. However, it would have been nice to have a replication package, since this would make verifiability easier (and avoid us the trouble). This is particularly troublesome since the whole point of the paper is to be quantitative, that is, making the best prediction. As we saw above, none of the precise estimates could be matched. What exactly is going on in their code? This flaw fault may be considerably more on our side than theirs, but we're unable to tell this -- even if we had all the time to recreate the zillions of the tables and models they provide, which we don't...

However, on top of that, the paper is frequently confusing in terms of the temporal choices for their variables. Specially, using the data that *they provided* we can't be reassured of the time periods for each of the variables -- more so on real economic measures. Also, probably trying to get more data by using a quarterly frequency, they made the paper more confusing then it would be so by using estimates on an annual basis. Nevertheless, probably in daily forecasting activities data forecast are done more on a quarterly basis, which gives practitioners good advice as how to perform it.

Finally, although they do perform some robustness checks, their choices for a numerous of periods to stop or specific models seemed kind of ad hoc. Many of these pitfalls could be solved by nowadays relative common machine learning techniques.
